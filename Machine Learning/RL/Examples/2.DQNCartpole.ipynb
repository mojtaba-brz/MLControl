{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from Lib.DQN import DQNAgent\n",
    "\n",
    "env_name = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(agent, N=500):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "\n",
    "    episode_lengths = np.zeros(shape=(N,))\n",
    "    mse_per_episode = np.zeros(shape=(N,))\n",
    "    scores = []\n",
    "    \n",
    "    batch_size = 64\n",
    "    max_steps = 400\n",
    "    converge_counter = 0\n",
    "    \n",
    "    for episode in range(N):\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])        # Reshape state for NN      \n",
    "        steps = 0\n",
    "        batch_loss = None\n",
    "        while steps < max_steps:\n",
    "            # Select and perform action\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])            # Reshape state for NN\n",
    "\n",
    "            # Modified reward structure\n",
    "            done = terminated or truncated\n",
    "            reward = reward if not done else -100\n",
    "            \n",
    "            # Store transition\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            # Perform batch training if enough samples\n",
    "            if len(agent.memory) > batch_size:\n",
    "                batch_loss = agent.replay(batch_size)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Store episode results\n",
    "        episode_lengths[episode] = steps\n",
    "        mse_per_episode[episode] = -1 if batch_loss is None else np.mean(batch_loss)\n",
    "        scores.append(reward)\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode: {episode + 1:4d} | \"\n",
    "                  f\"Score: {steps:4d} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.2f} | \"\n",
    "                  f\"Memory: {len(agent.memory):5d} | \"\n",
    "                  f\"MSE: {mse_per_episode[episode]:.2f}\")\n",
    "\n",
    "        if steps >= max_steps:\n",
    "            converge_counter += 1\n",
    "            if converge_counter >= N * 0.1:\n",
    "                break\n",
    "        \n",
    "    return agent, scores, episode_lengths, mse_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, render_mode='human')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n \n",
    "\n",
    "n_episodes = 100\n",
    "\n",
    "# agent_dqn = DQNAgent(state_size, action_size, alpha=0.001, use_double_learning=False) # Failed!\n",
    "# agent_dqn, reward_hist_tot1, ep_length_tot1, mse_per_episode_tot1 = single_run(agent_dqn, N=n_episodes) # Failed!\n",
    "agent_ddqn = DQNAgent(state_size, action_size, alpha=0.001, use_double_learning=True, target_model_update_freq=40)\n",
    "agent_ddqn, reward_hist_tot1, ep_length_tot1, mse_per_episode_tot1 = single_run(agent_ddqn, N=n_episodes)\n",
    "agent_d3qn = DQNAgent(state_size, action_size, alpha=0.001, use_double_learning=True, target_model_update_freq=40, use_dueling_net=True)\n",
    "agent_d3qn, reward_hist_tot1, ep_length_tot1, mse_per_episode_tot1 = single_run(agent_d3qn, N=n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(model, env):\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    trunc = False\n",
    "    steps = 0\n",
    "    while not done and not trunc:\n",
    "        s = np.reshape(s, [1, 4]) \n",
    "        a = model.act(s)\n",
    "        s, _, done, trunc, _ = env.step(a)\n",
    "        env.render()   \n",
    "        steps += 1\n",
    "        if steps >= 200:\n",
    "            break\n",
    "            \n",
    "    print(\"reward =\", steps)\n",
    "\n",
    "# run_simulation(agent_dqn, env) # Failed!\n",
    "print(\"1\")\n",
    "run_simulation(agent_ddqn, env)   \n",
    "print(\"2\")\n",
    "run_simulation(agent_d3qn, env)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLC_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
