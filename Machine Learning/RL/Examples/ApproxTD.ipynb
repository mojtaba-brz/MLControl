{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f24e17",
   "metadata": {},
   "source": [
    "# Approximate Temporal Difference\n",
    "\n",
    "In here we use the following formulation to approximate the action-value function.\n",
    "\n",
    "$\n",
    "Q(x, u) = \\phi^T\\mu(x, u)\n",
    "$\n",
    "\n",
    "- For estimating $\\phi$, we'll use SGD method.\n",
    "- For $\\mu$ in here we'll use RBFSampler from sklearn.kernel_approximation.\n",
    "\n",
    "**Note:** It seems that it doesn't work!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b865692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm \n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "from Lib.BasicPolicyRelatedTools import EpsilonGreedyPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinModel:\n",
    "    def __init__(self, env:gym.Env, policy : any, alpha = 0.05):\n",
    "        samples = self.gather_samples(env, 10000)\n",
    "        self.mu = RBFSampler()\n",
    "        self.mu.fit(samples)\n",
    "        \n",
    "        self.n_mu     = self.mu.n_components\n",
    "        self.n_action = env.action_space.n\n",
    "        \n",
    "        self.phi = np.zeros(self.n_mu)\n",
    "        self.policy = policy\n",
    "        self.alpha = alpha\n",
    "        pass\n",
    "    \n",
    "    def get_action_values(self, state):\n",
    "        Q_out = []\n",
    "        for a in range(self.n_action):\n",
    "            Q_out += [self.get_action_value(state, a)]\n",
    "        return Q_out\n",
    "    \n",
    "    def get_action_value(self, state, action):\n",
    "        state_action_vec = np.concatenate((state, [action]))\n",
    "        return self.phi.dot(self.mu.transform([state_action_vec])[0])\n",
    "    \n",
    "    def get_max_action_value(self, state):\n",
    "        Q_max = -np.inf\n",
    "        for a in range(self.n_action):\n",
    "            Q = self.get_action_value(state, a)\n",
    "            Q_max = Q if Q > Q_max else Q_max\n",
    "        return Q_max\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return self.policy.get_action(self.get_action_values(state))\n",
    "    \n",
    "    def update_step(self, state, action, target):\n",
    "        # target = r + gamma * max(Q(x, ...))\n",
    "        td_error = target - self.get_action_value(state, action)\n",
    "        self.phi += self.alpha * td_error * self.get_grad(state, action)\n",
    "    \n",
    "    def gather_samples(self, env, n_episodes=1000):\n",
    "        samples = []\n",
    "        for _ in tqdm(range(n_episodes), desc=\"Gathering samples...   \"):\n",
    "            s, _ = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                a = env.action_space.sample()\n",
    "                sa = np.concatenate((s, [a]))\n",
    "                samples.append(sa)\n",
    "                s, r, done, _, _ = env.step(a)\n",
    "        return samples\n",
    "    \n",
    "    def get_grad(self, state, action):\n",
    "        state_action_vec = np.concatenate((state, [action]))\n",
    "        return self.mu.transform([state_action_vec])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f415ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "Q = QLinModel(env, EpsilonGreedyPolicy(0.1), 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b158ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_the_model(model : QLinModel, env, n_iter):\n",
    "    gamma = 0.9\n",
    "    \n",
    "    episode_reward     = np.zeros(n_iter)\n",
    "    episode_time_steps = np.zeros(n_iter)\n",
    "    \n",
    "    \n",
    "    for iter in tqdm(range(n_iter), desc=\"Training...   \"):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        trunc = False\n",
    "        while not done and not trunc and episode_time_steps[iter] < 10000:\n",
    "            a = model.get_action(s)\n",
    "            s_next, r, done, trunc, _ = env.step(a)\n",
    "            \n",
    "            target = r + model.get_max_action_value(s_next)\n",
    "            model.update_step(s, a, target)\n",
    "            \n",
    "            episode_reward[iter] += r\n",
    "            episode_time_steps[iter] += 1\n",
    "            \n",
    "            s = s_next\n",
    "    \n",
    "    return model, episode_reward, episode_time_steps\n",
    "\n",
    "def run_simulation(model : QLinModel, env):\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    trunc = False\n",
    "    while not done and not trunc:\n",
    "        a = model.get_action(s)\n",
    "        _, _, done, trunc, _ = env.step(a)\n",
    "        env.render()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.alpha = 0.6\n",
    "for i in range(10):\n",
    "    Q, rewards, _ = train_the_model(Q, env, 150)\n",
    "    Q, rewards, _ = train_the_model(Q, env, 50)\n",
    "    print(f\"{i+1}. mean reward : {np.mean(rewards)}, alpha:{Q.alpha}\")\n",
    "    Q.alpha = np.max((0.4, Q.alpha*0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc655d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_simulation(Q, gym.make(\"CartPole-v1\", render_mode=\"human\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLC_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
