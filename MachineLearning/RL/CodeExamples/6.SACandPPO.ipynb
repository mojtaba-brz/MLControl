{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Soft Policy Gradient Methods\n",
    "\n",
    "#### Refs: \n",
    "- [Sharif University of Technology - Deep Reinforcement Learning (Fall 2024) - Dr.A.Emami and M.Narimani](https://github.com/mnarimani/DRL_Fall2024_SUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from Lib.ReplayBuffer import ReplayBuffer2\n",
    "from Lib.ActorCritic import ActorNetwork, CriticNetwork\n",
    "\n",
    "env_name = 'Pendulum-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(self, alpha=0.0003, beta=0.0003, input_dims=[8],\n",
    "            env=None, gamma=0.99, n_actions=2, max_size=1000000, tau=0.005,\n",
    "            layer1_size=256, layer2_size=256, batch_size=256, reward_scale=2):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer2(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Initialize temperature parameter (alpha) and its optimizer\n",
    "        self.log_alpha = tf.Variable(tf.math.log(0.2), dtype=tf.float32)\n",
    "        self.alpha_T = tf.exp(self.log_alpha)\n",
    "        self.alpha_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "        # Optional: Set target entropy (can also be set in learn method)\n",
    "        self.target_entropy = -tf.cast(self.n_actions, dtype=tf.float32)\n",
    "\n",
    "        self.actor = ActorNetwork(env, (250, 250), name='actor')\n",
    "        self.critic_1 = CriticNetwork(env, (256, 256), name='critic_1')\n",
    "        self.critic_2 = CriticNetwork(env, (256, 256), name='critic_2')\n",
    "        self.value = CriticNetwork(env, (256, 256), name='value')\n",
    "        self.target_value = CriticNetwork(env, (256, 256), name='target_value')\n",
    "\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
    "        self.critic_1.compile(optimizer=Adam(learning_rate=beta))\n",
    "        self.critic_2.compile(optimizer=Adam(learning_rate=beta))\n",
    "        self.value.compile(optimizer=Adam(learning_rate=beta))\n",
    "        self.target_value.compile(optimizer=Adam(learning_rate=beta))\n",
    "\n",
    "        self.scale = reward_scale\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
    "\n",
    "        return actions[0]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store(state, action, reward, new_state, done)\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_value.weights\n",
    "        for i, weight in enumerate(self.value.weights):\n",
    "            weights.append(weight * tau + targets[i]*(1-tau))\n",
    "\n",
    "        self.target_value.set_weights(weights)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.m_cntr < self.batch_size:\n",
    "            return 0, 0, 0, 0\n",
    "\n",
    "        state, action, reward, new_state, done = \\\n",
    "                self.memory.sample(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        # Update temperature parameter\n",
    "\n",
    "        # target_entropy = -tf.cast(tf.shape(action)[-1], dtype=tf.float32)\n",
    "        # with tf.GradientTape() as tape:\n",
    "        #     _, log_probs = self.actor.sample_normal(states, reparameterize=True)\n",
    "        #     log_probs = tf.squeeze(log_probs, 1)\n",
    "        #     alpha_loss = -tf.reduce_mean(\n",
    "        #         self.log_alpha * tf.stop_gradient(log_probs + target_entropy))\n",
    "        #\n",
    "        # alpha_gradient = tape.gradient(alpha_loss, [self.log_alpha])\n",
    "        # self.alpha_optimizer.apply_gradients(zip(alpha_gradient, [self.log_alpha]))\n",
    "        # self.alpha_T = tf.exp(self.log_alpha)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = self.value(states)\n",
    "            value_ = self.target_value(states_)\n",
    "\n",
    "            current_policy_actions, log_probs = self.actor.sample_normal(states,\n",
    "                                                        reparameterize=False)\n",
    "            log_probs = tf.squeeze(log_probs,1)\n",
    "            x = tf.concat([states, current_policy_actions], axis=-1)\n",
    "            q1_new_policy = self.critic_1(x)\n",
    "            q2_new_policy = self.critic_2(x)\n",
    "            critic_value = tf.squeeze(tf.math.minimum(q1_new_policy, q2_new_policy))\n",
    "\n",
    "            value_target = critic_value - self.alpha_T * log_probs\n",
    "            value_loss = 0.5 * keras.losses.MSE(value, value_target)\n",
    "\n",
    "        value_network_gradient = tape.gradient(value_loss, \n",
    "                                                self.value.trainable_variables)\n",
    "        self.value.optimizer.apply_gradients(zip(\n",
    "                       value_network_gradient, self.value.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_policy_actions, log_probs = self.actor.sample_normal(states,\n",
    "                                                reparameterize=True)\n",
    "            log_probs = tf.squeeze(log_probs, 1)\n",
    "            x = tf.concat([states, new_policy_actions], axis=-1)\n",
    "            q1_new_policy = self.critic_1(x)\n",
    "            q2_new_policy = self.critic_2(x)\n",
    "            critic_value = tf.math.minimum(q1_new_policy, q2_new_policy)\n",
    "        \n",
    "            actor_loss = self.alpha_T * log_probs - critic_value\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "            log_probs_2 = log_probs\n",
    "\n",
    "        actor_network_gradient = tape.gradient(actor_loss, \n",
    "                                            self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(\n",
    "                        actor_network_gradient, self.actor.trainable_variables))\n",
    "        \n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            q_hat = self.scale*reward + self.gamma*value_*(1-done)\n",
    "            x = tf.concat([state, action], axis=-1)\n",
    "            q1_old_policy = self.critic_1(x)\n",
    "            q2_old_policy = self.critic_2(x)\n",
    "            critic_1_loss = 0.5 * keras.losses.MSE(q1_old_policy, q_hat)\n",
    "            critic_2_loss = 0.5 * keras.losses.MSE(q2_old_policy, q_hat)\n",
    "    \n",
    "        critic_1_network_gradient = tape.gradient(critic_1_loss,\n",
    "                                        self.critic_1.trainable_variables)\n",
    "        critic_2_network_gradient = tape.gradient(critic_2_loss,\n",
    "            self.critic_2.trainable_variables)\n",
    "\n",
    "        self.critic_1.optimizer.apply_gradients(zip(\n",
    "            critic_1_network_gradient, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(\n",
    "            critic_2_network_gradient, self.critic_2.trainable_variables))\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "        return value_loss, actor_loss, critic_1_loss, critic_2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "agent = SACAgent(input_dims=env.observation_space.shape, env=env,\n",
    "              n_actions=env.action_space.shape[0])\n",
    "n_episodes = 150\n",
    "\n",
    "score_history = []\n",
    "value_losses = []\n",
    "actor_losses = []\n",
    "critic_1_losses = []\n",
    "critic_2_losses = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    while not done and steps < 200:\n",
    "        steps += 1\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        observation_, reward, done, info, _ = env.step(action.numpy() * 2)  # action output is in [-1,1]\n",
    "        score += reward\n",
    "        agent.remember(observation, action, reward, observation_, done)\n",
    "        value_loss, actor_loss, critic_1_loss, critic_2_loss = agent.learn()\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "\n",
    "    value_losses.append(value_loss)\n",
    "    actor_losses.append(actor_loss)\n",
    "    critic_1_losses.append(critic_1_loss)\n",
    "    critic_2_losses.append(critic_2_loss)\n",
    "\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if (i+1) % 5 == 0:\n",
    "        print(f\"Episode: {i+1:4d} | \"\n",
    "              f\"Score: {int(score):5d} | \"\n",
    "              f\"Avg Score: {int(avg_score):5d} | \"\n",
    "              f\"Actor Loss: {actor_loss:.2f} | \"\n",
    "              f\"Critic 1 Loss: {critic_1_loss:.2f} | \"\n",
    "              f\"Critic 2 Loss: {critic_2_loss:.2f} | \"\n",
    "              f\"Value Loss: {value_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(agent):\n",
    "    num_timesteps = 200\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = \"Renders/SAC_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        # action = env.action_space.sample()\n",
    "        action = agent.choose_action(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### PPO :   Proximal Policy Optimization (PPO-Clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "env = gym.make(env_name)\n",
    "S_DIM = env.observation_space.shape[0]\n",
    "A_DIM = env.action_space.shape[0]\n",
    "A_BOUND = [env.action_space.low[0], env.action_space.high[0]]\n",
    "EP_MAX = 2000\n",
    "EP_LEN = 200\n",
    "GAMMA = 0.9\n",
    "A_LR = 0.0001\n",
    "C_LR = 0.0005\n",
    "BATCH = 64\n",
    "A_UPDATE_STEPS = 10\n",
    "C_UPDATE_STEPS = 10\n",
    "EPSILON = 0.2 # Clipped surrogate objective\n",
    "\n",
    "class PPOActorNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.mu = tf.keras.layers.Dense(A_DIM, activation='tanh')\n",
    "        self.sigma = tf.keras.layers.Dense(A_DIM, activation='softplus')\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        mu = self.mu(x) * A_BOUND[1]\n",
    "        sigma = self.sigma(x)\n",
    "        return mu, sigma + 1e-4\n",
    "    \n",
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.value = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        return self.value(x)\n",
    "        \n",
    "class PPO:\n",
    "    def __init__(self):\n",
    "        self.actor = PPOActorNetwork()\n",
    "        self.actor_old = PPOActorNetwork()\n",
    "        self.critic = CriticNetwork()\n",
    "        \n",
    "        # Build models with dummy input\n",
    "        dummy_state = tf.random.normal((1, S_DIM))\n",
    "        self.actor(dummy_state)\n",
    "        self.actor_old(dummy_state)\n",
    "        self.critic(dummy_state)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=A_LR)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=C_LR)\n",
    "        \n",
    "        # Add metrics tracking\n",
    "        self.actor_loss_metric = tf.keras.metrics.Mean('actor_loss', dtype=tf.float32)\n",
    "        self.critic_loss_metric = tf.keras.metrics.Mean('critic_loss', dtype=tf.float32)\n",
    "\n",
    "    def update_old_actor(self):\n",
    "        self.actor_old.set_weights(self.actor.get_weights())\n",
    "\n",
    "    @tf.function\n",
    "    def choose_action(self, state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        mu, sigma = self.actor(state)\n",
    "        dist = tf.random.normal(shape=mu.shape)\n",
    "        action = mu + sigma * dist\n",
    "        return tf.clip_by_value(action[0], A_BOUND[0], A_BOUND[1])\n",
    "\n",
    "    @tf.function\n",
    "    def get_value(self, state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        return self.critic(state)[0, 0]\n",
    "\n",
    "    @tf.function\n",
    "    def actor_loss(self, states, actions, advantages):\n",
    "        mu, sigma = self.actor(states)\n",
    "        old_mu, old_sigma = self.actor_old(states)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        dist = tfp.distributions.Normal(mu, sigma)\n",
    "        old_dist = tfp.distributions.Normal(old_mu, old_sigma)\n",
    "        \n",
    "        ratio = tf.exp(dist.log_prob(actions) - old_dist.log_prob(actions))\n",
    "        surr = ratio * advantages\n",
    "        \n",
    "        # Clipped surrogate objective\n",
    "        clip_surr = tf.clip_by_value(ratio, 1.-EPSILON, 1.+EPSILON) * advantages\n",
    "        \n",
    "        return -tf.reduce_mean(tf.minimum(surr, clip_surr))\n",
    "\n",
    "    @tf.function\n",
    "    def critic_loss(self, states, discounted_rewards):\n",
    "        values = self.critic(states)\n",
    "        return tf.reduce_mean(tf.square(discounted_rewards - values))\n",
    "\n",
    "    @tf.function\n",
    "    def train_actor(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.actor_loss(states, actions, advantages)\n",
    "        grads = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "        self.actor_loss_metric.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_critic(self, states, discounted_rewards):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.critic_loss(states, discounted_rewards)\n",
    "        grads = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "        self.critic_loss_metric.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    def update(self, states, actions, rewards):\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        \n",
    "        # Reset metrics\n",
    "        self.actor_loss_metric.reset_state()\n",
    "        self.critic_loss_metric.reset_state()\n",
    "        \n",
    "        # Calculate advantage\n",
    "        values = self.critic(states)\n",
    "        advantages = rewards - values\n",
    "        \n",
    "        # Update old actor\n",
    "        self.update_old_actor()\n",
    "        \n",
    "        # Update actor\n",
    "        for _ in range(A_UPDATE_STEPS):\n",
    "            self.train_actor(states, actions, advantages)\n",
    "            \n",
    "        # Update critic\n",
    "        for _ in range(C_UPDATE_STEPS):\n",
    "            self.train_critic(states, rewards)\n",
    "            \n",
    "        # Return the average losses\n",
    "        return {\n",
    "            'actor_loss': self.actor_loss_metric.result().numpy(),\n",
    "            'critic_loss': self.critic_loss_metric.result().numpy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:   10 | Score: -1355 | Avg Score: -1337 | Actor Loss: 15.57 | Critic Loss: 430.60\n",
      "Episode:   20 | Score: -1228 | Avg Score: -1288 | Actor Loss: 1.56 | Critic Loss: 92.47\n",
      "Episode:   30 | Score: -1174 | Avg Score: -1285 | Actor Loss: -0.12 | Critic Loss: 107.26\n",
      "Episode:   40 | Score: -1224 | Avg Score: -1270 | Actor Loss: 10.12 | Critic Loss: 231.58\n",
      "Episode:   50 | Score: -1270 | Avg Score: -1258 | Actor Loss: 4.40 | Critic Loss: 151.51\n",
      "Episode:   60 | Score: -1222 | Avg Score: -1248 | Actor Loss: 0.78 | Critic Loss: 27.52\n",
      "Episode:   70 | Score: -1038 | Avg Score: -1235 | Actor Loss: -4.51 | Critic Loss: 63.09\n",
      "Episode:   80 | Score: -1194 | Avg Score: -1233 | Actor Loss: -0.44 | Critic Loss: 37.30\n",
      "Episode:   90 | Score: -1051 | Avg Score: -1222 | Actor Loss: 0.88 | Critic Loss: 45.49\n",
      "Episode:  100 | Score: -1068 | Avg Score: -1219 | Actor Loss: -3.71 | Critic Loss: 48.60\n",
      "Episode:  110 | Score: -1325 | Avg Score: -1201 | Actor Loss: 0.37 | Critic Loss: 48.76\n",
      "Episode:  120 | Score: -1187 | Avg Score: -1196 | Actor Loss: 0.76 | Critic Loss: 43.15\n",
      "Episode:  130 | Score: -1020 | Avg Score: -1183 | Actor Loss: -1.75 | Critic Loss: 69.20\n",
      "Episode:  140 | Score: -1068 | Avg Score: -1172 | Actor Loss: 2.53 | Critic Loss: 65.15\n",
      "Episode:  150 | Score: -1145 | Avg Score: -1155 | Actor Loss: 2.74 | Critic Loss: 42.03\n",
      "Episode:  160 | Score: -1121 | Avg Score: -1139 | Actor Loss: 3.11 | Critic Loss: 26.87\n",
      "Episode:  170 | Score:  -976 | Avg Score: -1123 | Actor Loss: -0.53 | Critic Loss: 9.40\n",
      "Episode:  180 | Score: -1033 | Avg Score: -1110 | Actor Loss: 2.70 | Critic Loss: 36.93\n",
      "Episode:  190 | Score: -1185 | Avg Score: -1104 | Actor Loss: 0.26 | Critic Loss: 16.20\n",
      "Episode:  200 | Score: -1305 | Avg Score: -1087 | Actor Loss: 0.70 | Critic Loss: 9.54\n",
      "Episode:  210 | Score:  -760 | Avg Score: -1076 | Actor Loss: -1.36 | Critic Loss: 15.58\n",
      "Episode:  220 | Score: -1001 | Avg Score: -1063 | Actor Loss: 0.54 | Critic Loss: 6.55\n",
      "Episode:  230 | Score:  -640 | Avg Score: -1048 | Actor Loss: -0.54 | Critic Loss: 29.50\n",
      "Episode:  240 | Score: -1378 | Avg Score: -1047 | Actor Loss: 0.91 | Critic Loss: 12.96\n",
      "Episode:  250 | Score:  -769 | Avg Score: -1019 | Actor Loss: 0.83 | Critic Loss: 7.82\n",
      "Episode:  260 | Score: -1063 | Avg Score: -1014 | Actor Loss: 1.04 | Critic Loss: 6.62\n",
      "Episode:  270 | Score:  -655 | Avg Score: -1003 | Actor Loss: -0.96 | Critic Loss: 23.27\n",
      "Episode:  280 | Score:  -750 | Avg Score:  -992 | Actor Loss: -1.86 | Critic Loss: 13.52\n",
      "Episode:  290 | Score:  -773 | Avg Score:  -997 | Actor Loss: -0.01 | Critic Loss: 5.77\n",
      "Episode:  300 | Score:  -912 | Avg Score:  -972 | Actor Loss: 3.81 | Critic Loss: 21.41\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m ba = np.vstack(buffer_a)\n\u001b[32m     35\u001b[39m br = np.array(discounted_r)[:, np.newaxis]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m losses = \u001b[43mppo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m ep_actor_losses.append(losses[\u001b[33m'\u001b[39m\u001b[33mactor_loss\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     39\u001b[39m ep_critic_losses.append(losses[\u001b[33m'\u001b[39m\u001b[33mcritic_loss\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mPPO.update\u001b[39m\u001b[34m(self, states, actions, rewards)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Update critic\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(C_UPDATE_STEPS):\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Return the average losses\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    141\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mactor_loss\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.actor_loss_metric.result().numpy(),\n\u001b[32m    142\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcritic_loss\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.critic_loss_metric.result().numpy()\n\u001b[32m    143\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    866\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    867\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    868\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    873\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    874\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    875\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ppo = PPO()\n",
    "all_ep_r = []\n",
    "all_actor_losses = []\n",
    "all_critic_losses = []\n",
    "\n",
    "for ep in range(EP_MAX):\n",
    "    s = env.reset()[0]\n",
    "    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "    ep_r = 0\n",
    "    ep_actor_losses = []\n",
    "    ep_critic_losses = []\n",
    "    \n",
    "    for t in range(EP_LEN):\n",
    "        a = ppo.choose_action(s).numpy()\n",
    "        s_, r, terminated, truncated, _ = env.step(a)\n",
    "        \n",
    "        buffer_s.append(s)\n",
    "        buffer_a.append(a)\n",
    "        buffer_r.append(r)\n",
    "        \n",
    "        s = s_\n",
    "        ep_r += r\n",
    "\n",
    "        # update ppo\n",
    "        if (t + 1) % BATCH == 0 or t == EP_LEN - 1:\n",
    "            v_s_ = ppo.get_value(s_).numpy()\n",
    "            discounted_r = []\n",
    "            for r in buffer_r[::-1]:\n",
    "                v_s_ = r + GAMMA * v_s_\n",
    "                discounted_r.append(v_s_)\n",
    "            discounted_r.reverse()\n",
    "\n",
    "            bs = np.vstack(buffer_s)\n",
    "            ba = np.vstack(buffer_a)\n",
    "            br = np.array(discounted_r)[:, np.newaxis]\n",
    "            \n",
    "            losses = ppo.update(bs, ba, br)\n",
    "            ep_actor_losses.append(losses['actor_loss'])\n",
    "            ep_critic_losses.append(losses['critic_loss'])\n",
    "            buffer_s, buffer_a, buffer_r = [], [], []\n",
    "\n",
    "\n",
    "    all_ep_r.append(ep_r)\n",
    "    avg_score = np.mean(all_ep_r[-100:])\n",
    "    \n",
    "    avg_actor_loss = np.mean(ep_actor_losses) if ep_actor_losses else 0\n",
    "    avg_critic_loss = np.mean(ep_critic_losses) if ep_critic_losses else 0\n",
    "    all_actor_losses.append(avg_actor_loss)\n",
    "    all_critic_losses.append(avg_critic_loss)\n",
    "\n",
    "    if (ep+1) % 10 == 0:\n",
    "        print(f\"Episode: {ep+1:4d} | \"\n",
    "              f\"Score: {int(ep_r):5d} | \"\n",
    "              f\"Avg Score: {int(avg_score):5d} | \"\n",
    "              f\"Actor Loss: {avg_actor_loss:.2f} | \"\n",
    "              f\"Critic Loss: {avg_critic_loss:.2f}\")\n",
    "\n",
    "    # Check if solved\n",
    "    if len(all_ep_r) >= 100 and avg_score >= -300:\n",
    "        print(f'Problem solved in {ep+1} episodes')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(agent):\n",
    "    num_timesteps = 400\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = \"Renders/PPO_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        # action = env.action_space.sample()\n",
    "        action = agent.choose_action(state).numpy()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(ppo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCEnv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
