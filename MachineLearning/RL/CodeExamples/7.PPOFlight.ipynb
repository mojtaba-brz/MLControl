{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b44db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 19:55:45.416196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756571145.432756   18336 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756571145.437740   18336 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756571145.450582   18336 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756571145.450604   18336 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756571145.450607   18336 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756571145.450610   18336 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-30 19:55:45.454971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from Lib.PPO import PPOAgent\n",
    "from Lib.RenderTools import render_gym_env\n",
    "\n",
    "env_name = \"Pendulum-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc26849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode     30: Mean Reward: -767.44 | Actor Loss:  +74.66 | Critic Loss: +6185.31\n",
      "Episode     40: Mean Reward: -770.99 | Actor Loss:  +56.96 | Critic Loss: +3993.97\n",
      "Episode     50: Mean Reward: -771.34 | Actor Loss:  +73.97 | Critic Loss: +5931.43\n",
      "Episode     60: Mean Reward: -733.59 | Actor Loss:  +71.60 | Critic Loss: +5755.75\n",
      "Episode     70: Mean Reward: -772.23 | Actor Loss:  +37.88 | Critic Loss: +2214.63\n",
      "Episode     80: Mean Reward: -706.10 | Actor Loss:  +72.56 | Critic Loss: +5735.91\n",
      "Episode     90: Mean Reward: -785.15 | Actor Loss:  +67.30 | Critic Loss: +4978.69\n",
      "Episode    100: Mean Reward: -747.14 | Actor Loss:  +65.08 | Critic Loss: +4681.59\n",
      "Episode    110: Mean Reward: -740.34 | Actor Loss:  +44.05 | Critic Loss: +2299.37\n",
      "Episode    120: Mean Reward: -743.80 | Actor Loss:  +36.96 | Critic Loss: +1648.31\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "actor_params_dict = {   \"hidden_layer_sizes\": (64,64),\n",
    "                        \"is_deterministic\": False,\n",
    "                        \"min_stdev\": 1e-6,  # action noise\n",
    "                        \"mean_layer_activation_function\": \"tanh\",\n",
    "                        \"hidden_layers_activation_function\": \"relu\",\n",
    "                        \"learning_rate\": 0.0001}  # Learning rate for actor\n",
    "critic_params_dict = {  \"hidden_layer_sizes\": (64,64),\n",
    "                        \"output_layer_activation_function\": \"linear\",\n",
    "                        \"hidden_layers_activation_function\": \"relu\",\n",
    "                        \"learning_rate\": 0.0003}  # Learning rate for critic\n",
    "ppo = PPOAgent(env, actor_params_dict, critic_params_dict)\n",
    "ppo.step(env, mode='train', \n",
    "         max_n_episodes=2500, \n",
    "         max_steps_per_episode=100, \n",
    "         episode_reward_stop_criteria = -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_gym_env(ppo, env_name, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCEnv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
