{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Deterministic Policy Gradient Methods\n",
    "\n",
    "#### Refs: \n",
    "- [Sharif University of Technology - Deep Reinforcement Learning (Fall 2024) - Dr.A.Emami and M.Narimani](https://github.com/mnarimani/DRL_Fall2024_SUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 19:53:21.673489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756571001.860674   18062 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756571001.916107   18062 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756571002.274977   18062 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756571002.275010   18062 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756571002.275012   18062 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756571002.275015   18062 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-30 19:53:22.309187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from Lib.ActorCritic import ActorNetwork, CriticNetwork\n",
    "from Lib.ReplayBuffer import ReplayBuffer\n",
    "\n",
    "env_name = 'Pendulum-v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        hidden_sizes=(300,),\n",
    "        start_steps=1e4,\n",
    "        replay_size=int(1e4),\n",
    "        batch_size=100,\n",
    "        gamma=0.99,\n",
    "        decay=0.995,\n",
    "        mu_lr=1e-3,\n",
    "        q_lr=1e-3,\n",
    "        action_noise=0.1,\n",
    "        max_episode_length=200\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.decay = decay\n",
    "        self.batch_size = batch_size\n",
    "        self.action_noise = action_noise\n",
    "        self.start_steps = start_steps\n",
    "        self.max_episode_length = max_episode_length\n",
    "        \n",
    "        # Extract environment dimensions\n",
    "        self.num_states = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.shape[0]\n",
    "        self.action_max = self.env.action_space.high[0]\n",
    "        \n",
    "        # Create networks\n",
    "        self.actor = ActorNetwork(env, hidden_sizes, True)\n",
    "        self.critic = CriticNetwork(env, hidden_sizes)\n",
    "        self.target_actor = ActorNetwork(env, hidden_sizes, True)\n",
    "        self.target_critic = CriticNetwork(env, hidden_sizes)\n",
    "        \n",
    "        # Build networks (initialize weights)\n",
    "        dummy_state = tf.zeros([1, self.num_states])\n",
    "        dummy_action = tf.zeros([1, self.num_actions])\n",
    "        self.actor(dummy_state)\n",
    "        x = tf.concat([dummy_state, dummy_action], axis=-1)\n",
    "        self.critic(x)\n",
    "        self.target_actor(dummy_state)\n",
    "        self.target_critic(x)\n",
    "        \n",
    "        # Copy weights to target networks\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        # Create optimizers\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(mu_lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(q_lr)\n",
    "        \n",
    "        # Create replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(self.num_states, self.num_actions, replay_size)\n",
    "        \n",
    "    def get_action(self, s, noise_scale):\n",
    "        a = self.actor(tf.convert_to_tensor(s.reshape(1,-1), dtype=tf.float32))\n",
    "        a = a.numpy()[0]  # Convert to numpy array\n",
    "        a += noise_scale * np.random.randn(self.num_actions)\n",
    "        return np.clip(a, -self.action_max, self.action_max)\n",
    "    \n",
    "    @tf.function\n",
    "    def update(self, batch):\n",
    "        states = tf.convert_to_tensor(batch['s'], dtype=tf.float32)\n",
    "        states_next = tf.convert_to_tensor(batch['s2'], dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(batch['a'], dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(batch['r'], dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(batch['d'], dtype=tf.float32)\n",
    "        \n",
    "        # Update critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Target actions\n",
    "            target_actions = self.target_actor(states_next)\n",
    "            \n",
    "            # Target Q-values\n",
    "            x = tf.concat([states_next, target_actions], axis=-1)\n",
    "            target_q = self.target_critic(x)\n",
    "            \n",
    "            # Q targets\n",
    "            q_target = rewards + self.gamma * (1 - dones) * target_q\n",
    "            \n",
    "            # Current Q-values\n",
    "            x = tf.concat([states, actions], axis=-1)\n",
    "            q = self.critic(x)\n",
    "            \n",
    "            # Critic loss\n",
    "            critic_loss = tf.reduce_mean((q - q_target)**2)\n",
    "        \n",
    "        # Get critic gradients\n",
    "        critic_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        \n",
    "        # Apply critic gradients\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_gradients, self.critic.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        # Update actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Actor actions\n",
    "            actor_actions = self.actor(states)\n",
    "            \n",
    "            # Actor loss\n",
    "            x = tf.concat([states, actor_actions], axis=-1)\n",
    "            actor_loss = -tf.reduce_mean(self.critic(x))\n",
    "        \n",
    "        # Get actor gradients\n",
    "        actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        \n",
    "        # Apply actor gradients\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_gradients, self.actor.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        return critic_loss, actor_loss\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        # Update target networks using soft update\n",
    "        for target, main in zip(self.target_actor.variables, self.actor.variables):\n",
    "            target.assign(self.decay * target + (1 - self.decay) * main)\n",
    "        \n",
    "        for target, main in zip(self.target_critic.variables, self.critic.variables):\n",
    "            target.assign(self.decay * target + (1 - self.decay) * main)\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        returns = []\n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        num_steps = 0\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state, episode_return, episode_length = self.env.reset()[0], 0, 0\n",
    "            done = False\n",
    "            \n",
    "            while not (done or episode_length == self.max_episode_length):\n",
    "                if num_steps > self.start_steps:\n",
    "                    action = self.get_action(state, self.action_noise)\n",
    "                else:\n",
    "                    action = self.env.action_space.sample()\n",
    "                \n",
    "                num_steps += 1\n",
    "                if num_steps == 1:\n",
    "                    print(f\"Using random actions for the initial {self.start_steps} steps...\")\n",
    "                if num_steps == self.start_steps:\n",
    "                    print(f\"{self.start_steps} steps reached. Using agent actions from now on.\")\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                # Store transition\n",
    "                done_store = False if episode_length == self.max_episode_length else done\n",
    "                self.replay_buffer.store(state, action, reward, next_state, done_store)\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "                \n",
    "                # Update networks\n",
    "                if num_steps > self.batch_size and self.replay_buffer.size >= self.batch_size:\n",
    "                    batch = self.replay_buffer.sample_batch(self.batch_size)\n",
    "                    critic_loss, actor_loss = self.update(batch)\n",
    "                    critic_losses.append(critic_loss.numpy())\n",
    "                    actor_losses.append(actor_loss.numpy())\n",
    "                    self.update_target_networks()\n",
    "            \n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"Episode: {episode + 1:4d} | \"\n",
    "                      f\"Score: {int(episode_return):5d} | \"\n",
    "                      f\"Actor Loss: {actor_loss:.2f} | \"\n",
    "                      f\"Critic Loss: {critic_loss:.2f}\")\n",
    "            returns.append(episode_return)\n",
    "\n",
    "        \n",
    "        return returns, critic_losses, actor_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "agent = DDPG(env, gamma=0.99)\n",
    "returns, critic_losses, actor_losses = agent.train(num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(agent):\n",
    "    num_timesteps = 200\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = \"Renders/DDPG_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        action = agent.get_action(state, 0)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Twin Delayed DDPG (TD3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.ReplayBuffer import ReplayBuffer3\n",
    "\n",
    "env_name = 'Pendulum-v1'\n",
    "# env_name = 'MountainCarContinuous-v0'\n",
    "\n",
    "class TD3:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        hidden_sizes=(300,),\n",
    "        replay_size=int(1e4),\n",
    "        gamma=0.99,\n",
    "        decay=0.995,\n",
    "        mu_lr=1e-3,\n",
    "        q_lr=1e-3,\n",
    "        batch_size=100,\n",
    "        action_noise=0.1,\n",
    "        target_noise=0.2,\n",
    "        noise_clip=0.5,\n",
    "        policy_delay=2,\n",
    "        max_episode_length=200\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.decay = decay\n",
    "        self.batch_size = batch_size\n",
    "        self.action_noise = action_noise\n",
    "        self.target_noise = target_noise                # TD3-specific: noise added to target actions\n",
    "        self.noise_clip = noise_clip                    # TD3-specific: clipping of target noise\n",
    "        self.policy_delay = policy_delay                # TD3-specific: delayed policy updates\n",
    "        self.max_episode_length = max_episode_length\n",
    "        \n",
    "        # Extract environment dimensions\n",
    "        self.num_states = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.shape[0]\n",
    "        self.action_max = self.env.action_space.high[0]\n",
    "        \n",
    "        # Create networks\n",
    "        self.actor = ActorNetwork(env, hidden_sizes, True)\n",
    "        self.critic1 = CriticNetwork(env, hidden_sizes)\n",
    "        self.critic2 = CriticNetwork(env, hidden_sizes)\n",
    "        self.target_actor = ActorNetwork(env, hidden_sizes, True)\n",
    "        self.target_critic1 = CriticNetwork(env, hidden_sizes)\n",
    "        self.target_critic2 = CriticNetwork(env, hidden_sizes)\n",
    "        \n",
    "        # Build networks (initialize weights)\n",
    "        dummy_state = tf.zeros([1, self.num_states])\n",
    "        dummy_action = tf.zeros([1, self.num_actions])\n",
    "        self.actor(dummy_state)\n",
    "        x = tf.concat([dummy_state, dummy_action], axis=-1)\n",
    "        self.critic1(x)\n",
    "        self.critic2(x)\n",
    "        self.target_actor(dummy_state)\n",
    "        self.target_critic1(x)\n",
    "        self.target_critic2(x)\n",
    "        \n",
    "        # Copy weights to target networks\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "        \n",
    "        # Create optimizers\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(mu_lr)\n",
    "        self.critic1_optimizer = tf.keras.optimizers.Adam(q_lr)\n",
    "        self.critic2_optimizer = tf.keras.optimizers.Adam(q_lr)\n",
    "        \n",
    "        # Create replay buffer\n",
    "        self.replay_buffer = ReplayBuffer3(replay_size)\n",
    "        \n",
    "        # Initialize step counter for delayed policy updates\n",
    "        self.total_it = 0\n",
    "        \n",
    "    def get_action(self, s, noise_scale):\n",
    "        a = self.actor(tf.convert_to_tensor(s.reshape(1,-1), dtype=tf.float32))\n",
    "        a = a.numpy()[0]\n",
    "        a += noise_scale * np.random.randn(self.num_actions)\n",
    "        return np.clip(a, -self.action_max, self.action_max)\n",
    "    \n",
    "    # @tf.function\n",
    "    def update(self, batch):\n",
    "        states = tf.convert_to_tensor(batch['s'], dtype=tf.float32)\n",
    "        states_next = tf.convert_to_tensor(batch['s2'], dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(batch['a'], dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(batch['r'], dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(batch['d'], dtype=tf.float32)\n",
    "        \n",
    "        # Add noise to target actions\n",
    "        noise = tf.random.normal(tf.shape(actions), stddev=self.target_noise)\n",
    "        noise = tf.clip_by_value(noise, -self.noise_clip, self.noise_clip)\n",
    "        \n",
    "        target_actions = self.target_actor(states_next)\n",
    "        target_actions = tf.clip_by_value(\n",
    "            target_actions + noise,\n",
    "            -self.action_max,\n",
    "            self.action_max\n",
    "        )\n",
    "        \n",
    "        # Get minimum Q-value between two critics\n",
    "        x = tf.concat([states_next, target_actions], axis=-1)\n",
    "        target_q1 = self.target_critic1(x)\n",
    "        target_q2 = self.target_critic2(x)\n",
    "        target_q = tf.minimum(target_q1, target_q2)\n",
    "        \n",
    "        # Q targets\n",
    "        q_target = rewards + self.gamma * (1 - dones) * target_q\n",
    "        \n",
    "        # Update first critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = tf.concat([states, actions], axis=-1)\n",
    "            q1 = self.critic1(x)\n",
    "            critic1_loss = tf.reduce_mean((q1 - q_target)**2)\n",
    "        \n",
    "        critic1_gradients = tape.gradient(critic1_loss, self.critic1.trainable_variables)\n",
    "        self.critic1_optimizer.apply_gradients(\n",
    "            zip(critic1_gradients, self.critic1.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        # Update second critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = tf.concat([states, actions], axis=-1)\n",
    "            q2 = self.critic2(x)\n",
    "            critic2_loss = tf.reduce_mean((q2 - q_target)**2)\n",
    "        \n",
    "        critic2_gradients = tape.gradient(critic2_loss, self.critic2.trainable_variables)\n",
    "        self.critic2_optimizer.apply_gradients(\n",
    "            zip(critic2_gradients, self.critic2.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_delay == 0:\n",
    "            # Update actor\n",
    "            with tf.GradientTape() as tape:\n",
    "                actor_actions = self.actor(states)\n",
    "                x = tf.concat([states, actor_actions], axis=-1)\n",
    "                actor_loss = -tf.reduce_mean(self.critic1(x))\n",
    "            \n",
    "            actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "            self.actor_optimizer.apply_gradients(\n",
    "                zip(actor_gradients, self.actor.trainable_variables)\n",
    "            )\n",
    "            \n",
    "            # Update target networks\n",
    "            self.update_target_networks()\n",
    "        else:\n",
    "            actor_loss = tf.constant(0.0)\n",
    "        \n",
    "        return critic1_loss, critic2_loss, actor_loss\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        # Update target networks using soft update\n",
    "        for target, main in zip(self.target_actor.variables, self.actor.variables):\n",
    "            target.assign(self.decay * target + (1 - self.decay) * main)\n",
    "        \n",
    "        for target, main in zip(self.target_critic1.variables, self.critic1.variables):\n",
    "            target.assign(self.decay * target + (1 - self.decay) * main)\n",
    "            \n",
    "        for target, main in zip(self.target_critic2.variables, self.critic2.variables):\n",
    "            target.assign(self.decay * target + (1 - self.decay) * main)\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        returns = []\n",
    "        # test_returns = []\n",
    "        critic1_losses = []\n",
    "        critic2_losses = []\n",
    "        actor_losses = []\n",
    "\n",
    "        print(f\"Using random actions for the initial {self.replay_buffer.max_size} steps...\")\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, episode_return, episode_length = self.env.reset()[0], 0, 0\n",
    "            done = False\n",
    "\n",
    "            while not (done or episode_length == self.max_episode_length):\n",
    "                # Use agent's actions only after buffer has enough samples\n",
    "                if len(self.replay_buffer) >= self.replay_buffer.max_size: #self.batch_size:\n",
    "                    action = self.get_action(state, self.action_noise)\n",
    "                else:\n",
    "                    action = self.env.action_space.sample()\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                # Store transition\n",
    "                done_store = False if episode_length == self.max_episode_length else done\n",
    "                self.replay_buffer.store(state, action, reward, next_state, done_store)\n",
    "                \n",
    "                if len(self.replay_buffer) == self.replay_buffer.max_size-1: #self.batch_size:\n",
    "                    print(f\"Memory full. Performing agent actions from now on.\")\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "                \n",
    "                # Update networks if buffer has enough samples\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    batch = self.replay_buffer.sample_batch(self.batch_size)\n",
    "                    critic1_loss, critic2_loss, actor_loss = self.update(batch)\n",
    "                    critic1_losses.append(critic1_loss.numpy())\n",
    "                    critic2_losses.append(critic2_loss.numpy())\n",
    "                    actor_losses.append(actor_loss.numpy())\n",
    "                    self.total_it += 1\n",
    "            \n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"Episode: {episode + 1:4d} | \"\n",
    "                      f\"Score: {int(episode_return):5d} | \"\n",
    "                      f\"Memory: {len(self.replay_buffer):5d} | \"\n",
    "                      f\"Actor Loss: {actor_loss.numpy():.2f} | \"\n",
    "                      f\"Critic 1 Loss: {critic1_loss.numpy():.2f} | \"\n",
    "                      f\"Critic 2 Loss: {critic2_loss.numpy():.2f}\")\n",
    "                \n",
    "            returns.append(episode_return)\n",
    "        \n",
    "        return returns, critic1_losses, critic2_losses, actor_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756571029.317627   18062 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4600 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random actions for the initial 10000 steps...\n",
      "Episode:   10 | Score: -1208 | Memory:  2000 | Actor Loss: 30.46 | Critic 1 Loss: 10.40 | Critic 2 Loss: 10.38\n",
      "Episode:   20 | Score: -1088 | Memory:  4000 | Actor Loss: 57.30 | Critic 1 Loss: 13.24 | Critic 2 Loss: 13.23\n",
      "Episode:   30 | Score: -1172 | Memory:  6000 | Actor Loss: 83.15 | Critic 1 Loss: 14.43 | Critic 2 Loss: 14.43\n",
      "Episode:   40 | Score: -1317 | Memory:  8000 | Actor Loss: 107.92 | Critic 1 Loss: 13.22 | Critic 2 Loss: 13.22\n",
      "Memory full. Performing agent actions from now on.\n",
      "Episode:   50 | Score:  -787 | Memory: 10000 | Actor Loss: 131.90 | Critic 1 Loss: 10.98 | Critic 2 Loss: 10.98\n",
      "Episode:   60 | Score: -1239 | Memory: 10000 | Actor Loss: 153.45 | Critic 1 Loss: 14.52 | Critic 2 Loss: 14.51\n",
      "Episode:   70 | Score: -1097 | Memory: 10000 | Actor Loss: 174.48 | Critic 1 Loss: 14.94 | Critic 2 Loss: 14.95\n",
      "Episode:   80 | Score: -1091 | Memory: 10000 | Actor Loss: 193.93 | Critic 1 Loss: 16.19 | Critic 2 Loss: 16.19\n",
      "Episode:   90 | Score: -1084 | Memory: 10000 | Actor Loss: 211.33 | Critic 1 Loss: 16.63 | Critic 2 Loss: 16.64\n",
      "Episode:  100 | Score: -1359 | Memory: 10000 | Actor Loss: 227.01 | Critic 1 Loss: 15.11 | Critic 2 Loss: 15.10\n",
      "Episode:  110 | Score:  -824 | Memory: 10000 | Actor Loss: 242.76 | Critic 1 Loss: 14.24 | Critic 2 Loss: 14.19\n",
      "Episode:  120 | Score:  -996 | Memory: 10000 | Actor Loss: 256.43 | Critic 1 Loss: 12.55 | Critic 2 Loss: 12.54\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "agent = TD3(env,gamma=0.99,policy_delay=2,target_noise=0.2,noise_clip=0.5)\n",
    "returns, critic1_losses, critic2_losses, actor_losses = agent.train(num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(agent):\n",
    "    num_timesteps = 200\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = \"Renders/TD3_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        action = agent.get_action(state, 0)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCEnv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
