{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods\n",
    "\n",
    "#### Refs: \n",
    "- [Sharif University of Technology - Deep Reinforcement Learning (Fall 2024) - Dr.A.Emami and M.Narimani](https://github.com/mnarimani/DRL_Fall2024_SUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from Lib.RBFFeature import FeatureTransformer\n",
    "from Lib.ActorCritic import ActorNetwork\n",
    "\n",
    "env_name = 'Pendulum-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ValueModel, self).__init__()\n",
    "        # initializer = #TODO: Add a kernel initializer for NN's initial weights\n",
    "        self.output_layer = keras.layers.Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        return tf.squeeze(self.output_layer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(env, feature_transformer, policy_model, value_model, gamma, max_steps):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    \n",
    "    while not done and iters < max_steps:\n",
    "        state = feature_transformer.transform(np.array([observation]))\n",
    "        action = policy_model.sample_action(state)\n",
    "        prev_observation = observation\n",
    "        observation, reward, done, trunc, _ = env.step(action.numpy()[0])\n",
    "        done = done or trunc\n",
    "        \n",
    "        totalreward += reward\n",
    "        if np.isnan(observation).any():\n",
    "            pass\n",
    "        \n",
    "        next_state = feature_transformer.transform(np.array([observation]))\n",
    "        V_next = value_model(next_state)\n",
    "        G = reward + gamma * V_next\n",
    "        advantage = G - value_model(state)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            mean, stdv = policy_model(state)\n",
    "            dist = tfp.distributions.Normal(mean, stdv)\n",
    "            log_prob = dist.log_prob(action)\n",
    "            loss = -tf.reduce_sum(advantage * log_prob + 0.01 * dist.entropy()) # TODO: try without including the regularization term\n",
    "            tf.debugging.assert_all_finite(loss, \"Loss is NaN/Inf\")\n",
    "\n",
    "        grads = tape.gradient(loss, policy_model.trainable_variables)\n",
    "        policy_model.optimizer.apply_gradients(zip(grads, policy_model.trainable_variables))\n",
    "        # for var in policy_model.trainable_variables:\n",
    "        #     tf.debugging.check_numerics(var, f\"NaN in {var.name}\")\n",
    "        if np.isnan(policy_model.trainable_variables[-1].value.numpy()[0]):\n",
    "            print(f\"b_std = {policy_model.trainable_variables[-1].value.numpy()[0]}, grad = {grads[-1].numpy()}\")\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            value_pred = value_model(state)\n",
    "            value_loss = tf.reduce_sum(tf.square(G - value_pred))\n",
    "        \n",
    "        grads = tape.gradient(value_loss, value_model.trainable_variables)\n",
    "        value_model.optimizer.apply_gradients(zip(grads, value_model.trainable_variables))\n",
    "\n",
    "        iters += 1\n",
    "    \n",
    "    mse_v = value_loss.numpy()\n",
    "    mae_pi = loss.numpy()\n",
    "    return totalreward, mse_v, mae_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(env, ft, num_episodes=150, lr_pi=1e-4, lr_val=1e-3, max_steps=200):\n",
    "    policy_model = ActorNetwork(env=env, hidden_layer_sizes=(), mean_layer_activation_function='linear')\n",
    "    value_model = ValueModel()\n",
    "    discount_rate = 0.96\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_mse_v = []\n",
    "    episode_mae_pi = []\n",
    "    \n",
    "    policy_model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_pi))\n",
    "    value_model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_val))\n",
    "    \n",
    "    for n in range(num_episodes):\n",
    "        total_reward, mse_v, mae_pi = play_one_episode(env, ft, policy_model, value_model, discount_rate, max_steps)\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_mse_v.append(mse_v)\n",
    "        episode_mae_pi.append(mae_pi)\n",
    "        if (n+1) % 10 == 0:\n",
    "            print(f\"Episode: {n+1:4d} | \"\n",
    "                  f\"Score: {int(total_reward):5d} | \"\n",
    "                  f\"Avg reward: {int(sum(episode_rewards)/len(episode_rewards)):5d} | \"\n",
    "                  f\"Policy MAE: {mae_pi:.2f} | \"\n",
    "                  f\"Value MSE: {mse_v:.2f}\")\n",
    "\n",
    "        # TODO: Add a convergence criteria\n",
    "\n",
    "    episode_rewards = np.array(episode_rewards)\n",
    "    episode_mse_v = np.array(episode_mse_v)\n",
    "    episode_mae_pi = np.array(episode_mae_pi)\n",
    "    \n",
    "    return episode_rewards, episode_mse_v, episode_mae_pi, policy_model, value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 150\n",
    "max_steps = 200\n",
    "num_components = 100\n",
    "env = gym.make(env_name)\n",
    "feature_transformer = FeatureTransformer(env, components_gammas=((num_components, 5),\n",
    "                                                                 (num_components, 2),\n",
    "                                                                 (num_components, 1),\n",
    "                                                                 (num_components, 0.5)), \n",
    "                                         n_samples=10000)\n",
    "\n",
    "rewards, mae_v, mae_pi, policy_model, value_model = run_training(\n",
    "    env,\n",
    "    feature_transformer,\n",
    "    num_episodes,\n",
    "    lr_pi=1e-3,\n",
    "    lr_val=1e-1,\n",
    "    max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(ft, agent):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = f\"Renders/AC_render_ENV-{env_name}.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(max_steps):\n",
    "        state = ft.transform(np.array([state]))\n",
    "        action = agent.sample_action(state)\n",
    "        state, _, done, _, _ = env.step([action.numpy()])\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(feature_transformer, policy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## A3C : Asynchronous Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_4 Epispde: 10 | Reward: -1386\n",
      "W_3 Epispde: 20 | Reward: -1566\n",
      "W_6 Epispde: 30 | Reward: -1322\n",
      "W_0 Epispde: 40 | Reward: -975\n",
      "W_4 Epispde: 50 | Reward: -1363\n",
      "W_3 Epispde: 60 | Reward: -1507\n",
      "W_6 Epispde: 70 | Reward: -1496\n",
      "W_0 Epispde: 80 | Reward: -1451\n",
      "W_4 Epispde: 90 | Reward: -1422\n",
      "W_7 Epispde: 100 | Reward: -1244\n",
      "W_6 Epispde: 110 | Reward: -1518\n",
      "W_0 Epispde: 120 | Reward: -986\n",
      "W_4 Epispde: 130 | Reward: -1317\n",
      "W_7 Epispde: 140 | Reward: -1523\n",
      "W_3 Epispde: 150 | Reward: -1253\n",
      "W_5 Epispde: 160 | Reward: -1699\n",
      "W_4 Epispde: 170 | Reward: -1427\n",
      "W_7 Epispde: 180 | Reward: -1133\n",
      "W_3 Epispde: 190 | Reward: -1269\n",
      "W_5 Epispde: 200 | Reward: -1426\n",
      "W_4 Epispde: 210 | Reward: -1426\n",
      "W_7 Epispde: 220 | Reward: -1388\n",
      "W_3 Epispde: 230 | Reward: -1087\n",
      "W_5 Epispde: 240 | Reward: -1221\n",
      "W_4 Epispde: 250 | Reward: -1448\n",
      "W_7 Epispde: 260 | Reward: -1532\n",
      "W_3 Epispde: 270 | Reward: -1280\n",
      "W_5 Epispde: 280 | Reward: -1181\n",
      "W_4 Epispde: 290 | Reward: -1494\n",
      "W_7 Epispde: 300 | Reward: -1318\n",
      "W_3 Epispde: 310 | Reward: -993\n",
      "W_5 Epispde: 320 | Reward: -873\n",
      "W_4 Epispde: 330 | Reward: -877\n",
      "W_6 Epispde: 340 | Reward: -1036\n",
      "W_3 Epispde: 350 | Reward: -1294\n",
      "W_5 Epispde: 360 | Reward: -1146\n",
      "W_0 Epispde: 370 | Reward: -1218\n",
      "W_6 Epispde: 380 | Reward: -1164\n",
      "W_3 Epispde: 390 | Reward: -1016\n",
      "W_5 Epispde: 400 | Reward: -1155\n",
      "W_2 Epispde: 410 | Reward: -1023\n",
      "W_6 Epispde: 420 | Reward: -1004\n",
      "W_3 Epispde: 430 | Reward: -961\n",
      "W_4 Epispde: 440 | Reward: -1263\n",
      "W_2 Epispde: 450 | Reward: -1168\n",
      "W_7 Epispde: 460 | Reward: -774\n",
      "W_0 Epispde: 470 | Reward: -1171\n",
      "W_4 Epispde: 480 | Reward: -883\n",
      "W_5 Epispde: 490 | Reward: -920\n",
      "W_7 Epispde: 500 | Reward: -1033\n",
      "W_0 Epispde: 510 | Reward: -936\n",
      "W_4 Epispde: 520 | Reward: -992\n",
      "W_6 Epispde: 530 | Reward: -513\n",
      "W_5 Epispde: 540 | Reward: -963\n",
      "W_0 Epispde: 550 | Reward: -872\n",
      "W_4 Epispde: 560 | Reward: -871\n",
      "W_6 Epispde: 570 | Reward: -950\n",
      "W_5 Epispde: 580 | Reward: -1016\n",
      "W_0 Epispde: 590 | Reward: -1388\n",
      "W_4 Epispde: 600 | Reward: -1220\n",
      "W_6 Epispde: 610 | Reward: -1265\n",
      "W_3 Epispde: 620 | Reward: -1108\n",
      "W_0 Epispde: 630 | Reward: -653\n",
      "W_4 Epispde: 640 | Reward: -1134\n",
      "W_2 Epispde: 650 | Reward: -644\n",
      "W_3 Epispde: 660 | Reward: -822\n",
      "W_0 Epispde: 670 | Reward: -1026\n",
      "W_4 Epispde: 680 | Reward: -789\n",
      "W_2 Epispde: 690 | Reward: -819\n",
      "W_3 Epispde: 700 | Reward: -1011\n",
      "W_0 Epispde: 710 | Reward: -965\n",
      "W_4 Epispde: 720 | Reward: -867\n",
      "W_2 Epispde: 730 | Reward: -649\n",
      "W_3 Epispde: 740 | Reward: -656\n",
      "W_4 Epispde: 750 | Reward: -612\n",
      "W_6 Epispde: 760 | Reward: -265\n",
      "W_0 Epispde: 770 | Reward: -501\n",
      "W_3 Epispde: 780 | Reward: -801\n",
      "W_5 Epispde: 790 | Reward: -531\n",
      "W_4 Epispde: 800 | Reward: -832\n",
      "W_3 Epispde: 810 | Reward: -392\n",
      "W_0 Epispde: 820 | Reward: -807\n",
      "W_7 Epispde: 830 | Reward: -266\n",
      "W_6 Epispde: 840 | Reward: -817\n",
      "W_3 Epispde: 850 | Reward: -1041\n",
      "W_7 Epispde: 860 | Reward: -560\n",
      "W_6 Epispde: 870 | Reward: -759\n",
      "W_0 Epispde: 880 | Reward: -409\n",
      "W_7 Epispde: 890 | Reward: -920\n",
      "W_5 Epispde: 900 | Reward: -125\n",
      "W_4 Epispde: 910 | Reward: -542\n",
      "W_7 Epispde: 920 | Reward: -148\n",
      "W_3 Epispde: 930 | Reward: -1\n",
      "W_5 Epispde: 940 | Reward: -540\n",
      "W_6 Epispde: 950 | Reward: -536\n",
      "W_4 Epispde: 960 | Reward: -134\n",
      "W_1 Epispde: 970 | Reward: -133\n",
      "W_7 Epispde: 980 | Reward: -550\n",
      "W_4 Epispde: 990 | Reward: -677\n",
      "W_0 Epispde: 1000 | Reward: -1\n",
      "W_2 Epispde: 1010 | Reward: -997\n",
      "W_1 Epispde: 1020 | Reward: -910\n",
      "W_4 Epispde: 1030 | Reward: -583\n",
      "W_0 Epispde: 1040 | Reward: -1230\n",
      "W_0 Epispde: 1050 | Reward: -268\n",
      "W_6 Epispde: 1060 | Reward: -273\n",
      "W_1 Epispde: 1070 | Reward: -682\n",
      "W_7 Epispde: 1080 | Reward: -396\n",
      "W_5 Epispde: 1090 | Reward: -799\n",
      "W_0 Epispde: 1100 | Reward: -412\n",
      "W_1 Epispde: 1110 | Reward: -261\n",
      "W_7 Epispde: 1120 | Reward: -3\n",
      "W_6 Epispde: 1130 | Reward: -133\n",
      "W_4 Epispde: 1140 | Reward: -687\n",
      "W_2 Epispde: 1150 | Reward: -547\n",
      "W_1 Epispde: 1160 | Reward: -136\n",
      "W_6 Epispde: 1170 | Reward: -132\n",
      "W_5 Epispde: 1180 | Reward: -133\n",
      "W_5 Epispde: 1190 | Reward: -132\n",
      "W_0 Epispde: 1200 | Reward: -131\n",
      "W_1 Epispde: 1210 | Reward: -133\n",
      "W_7 Epispde: 1220 | Reward: -404\n",
      "W_5 Epispde: 1230 | Reward: -685\n",
      "W_6 Epispde: 1240 | Reward: -553\n",
      "W_7 Epispde: 1250 | Reward: -678\n",
      "W_1 Epispde: 1260 | Reward: -628\n",
      "W_3 Epispde: 1270 | Reward: -1\n",
      "W_6 Epispde: 1280 | Reward: -584\n",
      "W_5 Epispde: 1290 | Reward: -580\n",
      "W_2 Epispde: 1300 | Reward: -682\n",
      "W_1 Epispde: 1310 | Reward: -802\n",
      "W_6 Epispde: 1320 | Reward: -626\n",
      "W_4 Epispde: 1330 | Reward: -397\n",
      "W_2 Epispde: 1340 | Reward: -271\n",
      "W_3 Epispde: 1350 | Reward: -543\n",
      "W_1 Epispde: 1360 | Reward: -544\n",
      "W_4 Epispde: 1370 | Reward: -266\n",
      "W_0 Epispde: 1380 | Reward: -798\n",
      "W_6 Epispde: 1390 | Reward: -2\n",
      "W_2 Epispde: 1400 | Reward: -539\n",
      "W_4 Epispde: 1410 | Reward: -133\n",
      "W_6 Epispde: 1420 | Reward: -600\n",
      "W_3 Epispde: 1430 | Reward: -133\n",
      "W_5 Epispde: 1440 | Reward: -682\n",
      "W_4 Epispde: 1450 | Reward: -404\n",
      "W_2 Epispde: 1460 | Reward: -572\n",
      "W_3 Epispde: 1470 | Reward: -603\n",
      "W_5 Epispde: 1480 | Reward: -572\n",
      "W_0 Epispde: 1490 | Reward: -135\n",
      "W_6 Epispde: 1500 | Reward: -762\n",
      "W_3 Epispde: 1510 | Reward: -455\n",
      "W_4 Epispde: 1520 | Reward: -130\n",
      "W_7 Epispde: 1530 | Reward: -578\n",
      "W_5 Epispde: 1540 | Reward: -756\n",
      "W_3 Epispde: 1550 | Reward: -416\n",
      "W_7 Epispde: 1560 | Reward: -638\n",
      "W_6 Epispde: 1570 | Reward: -650\n",
      "W_5 Epispde: 1580 | Reward: 0\n",
      "W_2 Epispde: 1590 | Reward: -557\n",
      "W_0 Epispde: 1600 | Reward: -551\n",
      "W_6 Epispde: 1610 | Reward: -553\n",
      "W_7 Epispde: 1620 | Reward: 0\n",
      "W_1 Epispde: 1630 | Reward: -665\n",
      "W_4 Epispde: 1640 | Reward: -1227\n",
      "W_2 Epispde: 1650 | Reward: 0\n",
      "W_0 Epispde: 1660 | Reward: -433\n",
      "W_4 Epispde: 1670 | Reward: -1309\n",
      "W_6 Epispde: 1680 | Reward: -1313\n",
      "W_1 Epispde: 1690 | Reward: -446\n",
      "W_3 Epispde: 1700 | Reward: -404\n",
      "W_7 Epispde: 1710 | Reward: -608\n",
      "W_4 Epispde: 1720 | Reward: -2\n",
      "W_1 Epispde: 1730 | Reward: -977\n",
      "W_7 Epispde: 1740 | Reward: -268\n",
      "W_5 Epispde: 1750 | Reward: -273\n",
      "W_4 Epispde: 1760 | Reward: -1110\n",
      "W_7 Epispde: 1770 | Reward: -1111\n",
      "W_3 Epispde: 1780 | Reward: -431\n",
      "W_1 Epispde: 1790 | Reward: -1\n",
      "W_5 Epispde: 1800 | Reward: -133\n",
      "W_5 Epispde: 1810 | Reward: -135\n",
      "W_6 Epispde: 1820 | Reward: -134\n",
      "W_7 Epispde: 1830 | Reward: -1221\n",
      "W_0 Epispde: 1840 | Reward: -426\n",
      "W_5 Epispde: 1850 | Reward: -1138\n",
      "W_7 Epispde: 1860 | Reward: -1209\n",
      "W_6 Epispde: 1870 | Reward: -3\n",
      "W_2 Epispde: 1880 | Reward: -130\n",
      "W_0 Epispde: 1890 | Reward: -433\n",
      "W_6 Epispde: 1900 | Reward: -1283\n",
      "W_5 Epispde: 1910 | Reward: -1396\n",
      "W_2 Epispde: 1920 | Reward: -1259\n",
      "W_6 Epispde: 1930 | Reward: -1399\n",
      "W_1 Epispde: 1940 | Reward: -1347\n",
      "W_7 Epispde: 1950 | Reward: -1420\n",
      "W_5 Epispde: 1960 | Reward: -1378\n",
      "W_6 Epispde: 1970 | Reward: -409\n",
      "W_7 Epispde: 1980 | Reward: -1274\n",
      "W_4 Epispde: 1990 | Reward: -298\n",
      "W_5 Epispde: 2000 | Reward: -1285\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from Lib.ActorCritic import ActorCritic\n",
    "# Environment setup\n",
    "env = gym.make(env_name)\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.shape[0]\n",
    "action_bound = [env.action_space.low, env.action_space.high]\n",
    "\n",
    "# Hyperparameters\n",
    "num_workers = 8 # cpu_count()\n",
    "num_episodes = 2000\n",
    "num_timesteps = 200\n",
    "global_net_scope = 'Global_Net'\n",
    "update_global = 10\n",
    "gamma = 0.90\n",
    "beta = 0.01\n",
    "\n",
    "class A3CWorker:\n",
    "    def __init__(self, name, global_ac):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.name = name\n",
    "        self.AC = ActorCritic(self.env, name, global_ac)\n",
    "    \n",
    "    def work(self):\n",
    "        global global_rewards, global_episodes\n",
    "        total_step = 1\n",
    "        \n",
    "        while global_episodes < num_episodes:\n",
    "            state, _ = self.env.reset()\n",
    "            batch_states, batch_actions, batch_rewards = [], [], []\n",
    "            Return = 0\n",
    "            \n",
    "            for t in range(num_timesteps):               \n",
    "                action = self.AC.get_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                \n",
    "                done = True if t == num_timesteps - 1 else False\n",
    "                Return += reward\n",
    "                \n",
    "                batch_states.append(state)\n",
    "                batch_actions.append(action)\n",
    "                batch_rewards.append(reward)\n",
    "                \n",
    "                if total_step % update_global == 0 or done:\n",
    "                    if done:\n",
    "                        target = 0\n",
    "                    else:\n",
    "                        target = self.AC.critic(tf.convert_to_tensor([next_state], dtype=tf.float32))\n",
    "                        target = target.numpy()[0, 0]\n",
    "                    \n",
    "                    batch_target_value = []\n",
    "                    for reward in batch_rewards[::-1]:\n",
    "                        target = reward + gamma * target\n",
    "                        batch_target_value.append(target)\n",
    "                    batch_target_value.reverse()\n",
    "                    \n",
    "                    self.AC.update(np.vstack(batch_states),\n",
    "                                   np.vstack(batch_actions),\n",
    "                                   np.vstack(batch_target_value))\n",
    "                    \n",
    "                    batch_states, batch_actions, batch_rewards = [], [], []\n",
    "                \n",
    "                state = next_state\n",
    "                total_step += 1\n",
    "                \n",
    "                if done:\n",
    "                    global_rewards.append(Return)\n",
    "                    global_episodes += 1\n",
    "                    if (global_episodes+1) % 10 == 0:\n",
    "                        print(\n",
    "                            self.name,\n",
    "                            \"Epispde:\", global_episodes+1,\n",
    "                            \"| Reward: %i\" % global_rewards[-1]\n",
    "                            )\n",
    "                    break\n",
    "\n",
    "# Training setup\n",
    "global_rewards = []\n",
    "global_episodes = 0\n",
    "\n",
    "# Initialize networks\n",
    "global_ac = ActorCritic(env, global_net_scope)\n",
    "workers = [A3CWorker(f'W_{i}', global_ac) for i in range(num_workers)]\n",
    "\n",
    "# Start worker threads\n",
    "worker_threads = []\n",
    "for worker in workers:\n",
    "    thread = threading.Thread(target=worker.work)\n",
    "    thread.start()\n",
    "    worker_threads.append(thread)\n",
    "    # worker.work()\n",
    "# Wait for all threads to complete\n",
    "for thread in worker_threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ML & Control\\2- Figure Out How To Use AI In Aerial Robotics (Learning Phase)\\MLControl\\MLCEnvWin\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"Renders/A3C_render.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(policy_model):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    video_path = \"Renders/A3C_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        action = policy_model.get_action(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(global_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## A2C : (Synchronous) Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "import threading\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from Lib.ActorCritic import ActorNetwork\n",
    "\n",
    "env_name = 'Pendulum-v1'\n",
    "\n",
    "from Lib.ActorCritic import ActorNetwork, CriticNetwork, GlobalAgent\n",
    "# Environment setup\n",
    "env = gym.make(env_name)\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.shape[0]\n",
    "action_bound = [env.action_space.low, env.action_space.high]\n",
    "\n",
    "# Hyperparameters\n",
    "num_workers = 8 # cpu_count()\n",
    "num_episodes = 2000\n",
    "num_timesteps = 200\n",
    "global_net_scope = 'Global_Net'\n",
    "update_global = 10\n",
    "gamma = 0.90\n",
    "beta = 0.01\n",
    "\n",
    "class A2CWorker:\n",
    "    def __init__(self, name, global_agent, gradient_queue):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.name = name\n",
    "        self.global_agent = global_agent\n",
    "        self.gradient_queue = gradient_queue\n",
    "        \n",
    "        # Local networks\n",
    "        self.actor = ActorNetwork(env, (250, 250))\n",
    "        self.critic = CriticNetwork(env)\n",
    "        \n",
    "        self.actor.build_networks()\n",
    "        self.critic.build_networks()\n",
    "        \n",
    "        self.sync_with_global()\n",
    "    \n",
    "    def sync_with_global(self):\n",
    "        \"\"\"Synchronize local networks with global networks\"\"\"\n",
    "        self.actor.set_weights(self.global_agent.actor.get_weights())\n",
    "        self.critic.set_weights(self.global_agent.critic.get_weights())\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        mean, stdev = self.actor(state)\n",
    "        stdev = stdev + 1e-8\n",
    "        \n",
    "        normal_dist = tf.random.normal(shape=mean.shape)\n",
    "        action = mean + stdev * normal_dist\n",
    "        action = tf.clip_by_value(action, action_bound[0], action_bound[1])\n",
    "        \n",
    "        return action[0]\n",
    "    \n",
    "    def compute_gradients(self, states, actions, target_values):\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        target_values = tf.convert_to_tensor(target_values, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Actor forward pass\n",
    "            mean, variance = self.actor(states)\n",
    "            mean = mean * action_bound[1]\n",
    "            variance = variance + 1e-4\n",
    "            \n",
    "            # Create normal distribution\n",
    "            dist = tfp.distributions.Normal(mean, tf.sqrt(variance))\n",
    "            log_prob = tf.reduce_sum(dist.log_prob(actions), axis=1, keepdims=True)\n",
    "            entropy = tf.reduce_sum(dist.entropy(), axis=1, keepdims=True)\n",
    "            \n",
    "            # Critic forward pass\n",
    "            values = self.critic(states)\n",
    "            td_error = target_values - values\n",
    "            \n",
    "            # Define losses\n",
    "            actor_loss = -tf.reduce_mean(log_prob * tf.stop_gradient(td_error) + beta * entropy)\n",
    "            critic_loss = tf.reduce_mean(tf.square(td_error))\n",
    "        \n",
    "        # Compute gradients\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        \n",
    "        # Clip gradients\n",
    "        actor_grads = [tf.clip_by_norm(grad, 40) for grad in actor_grads if grad is not None]\n",
    "        critic_grads = [tf.clip_by_norm(grad, 40) for grad in critic_grads if grad is not None]\n",
    "        \n",
    "        return actor_grads, critic_grads\n",
    "        \n",
    "    \n",
    "    def work(self, coordinator):\n",
    "        global global_rewards, global_episodes\n",
    "        episode = 0\n",
    "        \n",
    "        while episode < num_episodes and not coordinator.should_stop():\n",
    "            state, _ = self.env.reset()\n",
    "            batch_states, batch_actions, batch_rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "            \n",
    "            for t in range(num_timesteps):\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                \n",
    "                done = True if t == num_timesteps - 1 else False\n",
    "                episode_reward += reward\n",
    "                \n",
    "                batch_states.append(state)\n",
    "                batch_actions.append(action)\n",
    "                batch_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    if done:\n",
    "                        target = 0\n",
    "                    else:\n",
    "                        target = self.critic(tf.convert_to_tensor([next_state], dtype=tf.float32))\n",
    "                        target = target.numpy()[0, 0]\n",
    "                    \n",
    "                    # Target values\n",
    "                    batch_target_value = []\n",
    "                    for reward in batch_rewards[::-1]:\n",
    "                        target = reward + gamma * target\n",
    "                        batch_target_value.append(target)\n",
    "                    batch_target_value.reverse()\n",
    "                    \n",
    "                    # Compute gradients and add to queue\n",
    "                    gradients = self.compute_gradients(\n",
    "                        np.vstack(batch_states),\n",
    "                        np.vstack(batch_actions),\n",
    "                        np.vstack(batch_target_value)\n",
    "                    )\n",
    "                    self.gradient_queue.put(gradients)\n",
    "                    \n",
    "                    # Update episode stats\n",
    "                    global_rewards.append(episode_reward)\n",
    "                    episode += 1\n",
    "                    \n",
    "                    if (episode+1) % 10 == 0:\n",
    "                        print(\n",
    "                            self.name,\n",
    "                            \"Epispde:\", episode+1,\n",
    "                            \"| Reward: %i\" % global_rewards[-1]\n",
    "                            )\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "# Initialize global agent and gradient queue\n",
    "global_agent = GlobalAgent(env, (250, 250))\n",
    "gradient_queue = Queue()\n",
    "global_rewards = []\n",
    "global_episodes = 0\n",
    "\n",
    "# Create coordinator for managing workers\n",
    "coordinator = tf.train.Coordinator()\n",
    "\n",
    "# Initialize workers\n",
    "workers = [A2CWorker(f'W_{i}', global_agent, gradient_queue) \n",
    "          for i in range(num_workers)]\n",
    "\n",
    "# Create worker threads\n",
    "worker_threads = []\n",
    "for worker in workers:\n",
    "    thread = threading.Thread(target=worker.work, args=(coordinator,))\n",
    "    thread.start()\n",
    "    worker_threads.append(thread)\n",
    "    # worker.work(coordinator)\n",
    "    \n",
    "try:\n",
    "    while global_episodes < num_episodes:\n",
    "        # Wait for all workers to submit their gradients\n",
    "        all_gradients = []\n",
    "        for _ in range(num_workers):\n",
    "            gradients = gradient_queue.get()\n",
    "            all_gradients.append(gradients)\n",
    "        \n",
    "        # Perform synchronous update\n",
    "        global_agent.update(all_gradients)\n",
    "        \n",
    "        # Synchronize all workers with updated global networks\n",
    "        for worker in workers:\n",
    "            worker.sync_with_global()\n",
    "        \n",
    "        global_episodes += num_workers\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    coordinator.request_stop()\n",
    "\n",
    "coordinator.request_stop()\n",
    "coordinator.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(policy_model):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = \"Renders/A2C_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        action = policy_model.get_action(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(workers[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCEnvWin (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
