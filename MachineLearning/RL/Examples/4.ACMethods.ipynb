{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods\n",
    "\n",
    "#### Refs: \n",
    "- [Sharif University of Technology - Deep Reinforcement Learning (Fall 2024) - Dr.A.Emami and M.Narimani](https://github.com/mnarimani/DRL_Fall2024_SUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from Lib.RBFFeature import FeatureTransformer\n",
    "from Lib.ActorCritic import ActorNetwork\n",
    "\n",
    "env_name = 'Pendulum-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ValueModel, self).__init__()\n",
    "        # initializer = #TODO: Add a kernel initializer for NN's initial weights\n",
    "        self.output_layer = keras.layers.Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        return tf.squeeze(self.output_layer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(env, feature_transformer, policy_model, value_model, gamma, max_steps):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    \n",
    "    while not done and iters < max_steps:\n",
    "        state = feature_transformer.transform(np.array([observation]))\n",
    "        action = policy_model.sample_action(state)\n",
    "        prev_observation = observation\n",
    "        observation, reward, done, trunc, _ = env.step(action.numpy()[0])\n",
    "        done = done or trunc\n",
    "        \n",
    "        totalreward += reward\n",
    "        if np.isnan(observation).any():\n",
    "            pass\n",
    "        \n",
    "        next_state = feature_transformer.transform(np.array([observation]))\n",
    "        V_next = value_model(next_state)\n",
    "        G = reward + gamma * V_next\n",
    "        advantage = G - value_model(state)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            mean, stdv = policy_model(state)\n",
    "            dist = tfp.distributions.Normal(mean, stdv)\n",
    "            log_prob = dist.log_prob(action)\n",
    "            loss = -tf.reduce_sum(advantage * log_prob + 0.01 * dist.entropy()) # TODO: try without including the regularization term\n",
    "            tf.debugging.assert_all_finite(loss, \"Loss is NaN/Inf\")\n",
    "\n",
    "        grads = tape.gradient(loss, policy_model.trainable_variables)\n",
    "        policy_model.optimizer.apply_gradients(zip(grads, policy_model.trainable_variables))\n",
    "        # for var in policy_model.trainable_variables:\n",
    "        #     tf.debugging.check_numerics(var, f\"NaN in {var.name}\")\n",
    "        if np.isnan(policy_model.trainable_variables[-1].value.numpy()[0]):\n",
    "            print(f\"b_std = {policy_model.trainable_variables[-1].value.numpy()[0]}, grad = {grads[-1].numpy()}\")\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            value_pred = value_model(state)\n",
    "            value_loss = tf.reduce_sum(tf.square(G - value_pred))\n",
    "        \n",
    "        grads = tape.gradient(value_loss, value_model.trainable_variables)\n",
    "        value_model.optimizer.apply_gradients(zip(grads, value_model.trainable_variables))\n",
    "\n",
    "        iters += 1\n",
    "    \n",
    "    mse_v = value_loss.numpy()\n",
    "    mae_pi = loss.numpy()\n",
    "    return totalreward, mse_v, mae_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(env, ft, num_episodes=150, lr_pi=1e-4, lr_val=1e-3, max_steps=200):\n",
    "    policy_model = ActorNetwork(env=env, hidden_layer_sizes=(), mean_layer_activation_function='linear')\n",
    "    value_model = ValueModel()\n",
    "    discount_rate = 0.96\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_mse_v = []\n",
    "    episode_mae_pi = []\n",
    "    \n",
    "    policy_model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_pi))\n",
    "    value_model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_val))\n",
    "    \n",
    "    for n in range(num_episodes):\n",
    "        total_reward, mse_v, mae_pi = play_one_episode(env, ft, policy_model, value_model, discount_rate, max_steps)\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_mse_v.append(mse_v)\n",
    "        episode_mae_pi.append(mae_pi)\n",
    "        if (n+1) % 10 == 0:\n",
    "            print(f\"Episode: {n+1:4d} | \"\n",
    "                  f\"Score: {int(total_reward):5d} | \"\n",
    "                  f\"Avg reward: {int(sum(episode_rewards)/len(episode_rewards)):5d} | \"\n",
    "                  f\"Policy MAE: {mae_pi:.2f} | \"\n",
    "                  f\"Value MSE: {mse_v:.2f}\")\n",
    "\n",
    "        # TODO: Add a convergence criteria\n",
    "\n",
    "    episode_rewards = np.array(episode_rewards)\n",
    "    episode_mse_v = np.array(episode_mse_v)\n",
    "    episode_mae_pi = np.array(episode_mae_pi)\n",
    "    \n",
    "    return episode_rewards, episode_mse_v, episode_mae_pi, policy_model, value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 150\n",
    "max_steps = 200\n",
    "num_components = 100\n",
    "env = gym.make(env_name)\n",
    "feature_transformer = FeatureTransformer(env, components_gammas=((num_components, 5),\n",
    "                                                                 (num_components, 2),\n",
    "                                                                 (num_components, 1),\n",
    "                                                                 (num_components, 0.5)), \n",
    "                                         n_samples=10000)\n",
    "\n",
    "rewards, mae_v, mae_pi, policy_model, value_model = run_training(\n",
    "    env,\n",
    "    feature_transformer,\n",
    "    num_episodes,\n",
    "    lr_pi=1e-3,\n",
    "    lr_val=1e-1,\n",
    "    max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(ft, agent):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = f\"Renders/AC_render_ENV-{env_name}.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(max_steps):\n",
    "        state = ft.transform(np.array([state]))\n",
    "        action = agent.sample_action(state)\n",
    "        state, _, done, _, _ = env.step([action.numpy()])\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(feature_transformer, policy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## A3C : Asynchronous Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from Lib.ActorCritic import ActorCritic\n",
    "# Environment setup\n",
    "env = gym.make(env_name)\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.shape[0]\n",
    "action_bound = [env.action_space.low, env.action_space.high]\n",
    "\n",
    "# Hyperparameters\n",
    "num_workers = 8 # cpu_count()\n",
    "num_episodes = 2000\n",
    "num_timesteps = 200\n",
    "global_net_scope = 'Global_Net'\n",
    "update_global = 10\n",
    "gamma = 0.90\n",
    "beta = 0.01\n",
    "\n",
    "class A3CWorker:\n",
    "    def __init__(self, name, global_ac):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.name = name\n",
    "        self.AC = ActorCritic(self.env, name, global_ac)\n",
    "    \n",
    "    def work(self):\n",
    "        global global_rewards, global_episodes\n",
    "        total_step = 1\n",
    "        \n",
    "        while global_episodes < num_episodes:\n",
    "            state, _ = self.env.reset()\n",
    "            batch_states, batch_actions, batch_rewards = [], [], []\n",
    "            Return = 0\n",
    "            \n",
    "            for t in range(num_timesteps):               \n",
    "                action = self.AC.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                \n",
    "                done = True if t == num_timesteps - 1 else False\n",
    "                Return += reward\n",
    "                \n",
    "                batch_states.append(state)\n",
    "                batch_actions.append(action)\n",
    "                batch_rewards.append(reward)\n",
    "                \n",
    "                if total_step % update_global == 0 or done:\n",
    "                    if done:\n",
    "                        target = 0\n",
    "                    else:\n",
    "                        target = self.AC.critic(tf.convert_to_tensor([next_state], dtype=tf.float32))\n",
    "                        target = target.numpy()[0, 0]\n",
    "                    \n",
    "                    batch_target_value = []\n",
    "                    for reward in batch_rewards[::-1]:\n",
    "                        target = reward + gamma * target\n",
    "                        batch_target_value.append(target)\n",
    "                    batch_target_value.reverse()\n",
    "                    \n",
    "                    self.AC.update(np.vstack(batch_states),\n",
    "                                   np.vstack(batch_actions),\n",
    "                                   np.vstack(batch_target_value))\n",
    "                    \n",
    "                    batch_states, batch_actions, batch_rewards = [], [], []\n",
    "                \n",
    "                state = next_state\n",
    "                total_step += 1\n",
    "                \n",
    "                if done:\n",
    "                    global_rewards.append(Return)\n",
    "                    global_episodes += 1\n",
    "                    if (global_episodes+1) % 10 == 0:\n",
    "                        print(\n",
    "                            self.name,\n",
    "                            \"Epispde:\", global_episodes+1,\n",
    "                            \"| Reward: %i\" % global_rewards[-1]\n",
    "                            )\n",
    "                    break\n",
    "\n",
    "# Training setup\n",
    "global_rewards = []\n",
    "global_episodes = 0\n",
    "\n",
    "# Initialize networks\n",
    "global_ac = ActorCritic(env, global_net_scope)\n",
    "workers = [A3CWorker(f'W_{i}', global_ac) for i in range(num_workers)]\n",
    "\n",
    "# Start worker threads\n",
    "worker_threads = []\n",
    "for worker in workers:\n",
    "    thread = threading.Thread(target=worker.work)\n",
    "    thread.start()\n",
    "    worker_threads.append(thread)\n",
    "    # worker.work()\n",
    "# Wait for all threads to complete\n",
    "for thread in worker_threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(policy_model):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    video_path = \"Renders/A3C_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        action = policy_model.select_action(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(global_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## A2C : (Synchronous) Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "import threading\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from Lib.ActorCritic import ActorNetwork\n",
    "\n",
    "env_name = 'Pendulum-v1'\n",
    "\n",
    "from Lib.ActorCritic import ActorNetwork, CriticNetwork, GlobalAgent\n",
    "# Environment setup\n",
    "env = gym.make(env_name)\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.shape[0]\n",
    "action_bound = [env.action_space.low, env.action_space.high]\n",
    "\n",
    "# Hyperparameters\n",
    "num_workers = 8 # cpu_count()\n",
    "num_episodes = 2000\n",
    "num_timesteps = 200\n",
    "global_net_scope = 'Global_Net'\n",
    "update_global = 10\n",
    "gamma = 0.90\n",
    "beta = 0.01\n",
    "\n",
    "class A2CWorker:\n",
    "    def __init__(self, name, global_agent, gradient_queue):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.name = name\n",
    "        self.global_agent = global_agent\n",
    "        self.gradient_queue = gradient_queue\n",
    "        \n",
    "        # Local networks\n",
    "        self.actor = ActorNetwork(env, (250, 250))\n",
    "        self.critic = CriticNetwork(env)\n",
    "        \n",
    "        self.actor.build_networks()\n",
    "        self.critic.build_networks()\n",
    "        \n",
    "        self.sync_with_global()\n",
    "    \n",
    "    def sync_with_global(self):\n",
    "        \"\"\"Synchronize local networks with global networks\"\"\"\n",
    "        self.actor.set_weights(self.global_agent.actor.get_weights())\n",
    "        self.critic.set_weights(self.global_agent.critic.get_weights())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        mean, stdev = self.actor(state)\n",
    "        stdev = stdev + 1e-8\n",
    "        \n",
    "        normal_dist = tf.random.normal(shape=mean.shape)\n",
    "        action = mean + stdev * normal_dist\n",
    "        action = tf.clip_by_value(action, action_bound[0], action_bound[1])\n",
    "        \n",
    "        return action[0]\n",
    "    \n",
    "    def compute_gradients(self, states, actions, target_values):\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        target_values = tf.convert_to_tensor(target_values, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Actor forward pass\n",
    "            mean, variance = self.actor(states)\n",
    "            mean = mean * action_bound[1]\n",
    "            variance = variance + 1e-4\n",
    "            \n",
    "            # Create normal distribution\n",
    "            dist = tfp.distributions.Normal(mean, tf.sqrt(variance))\n",
    "            log_prob = tf.reduce_sum(dist.log_prob(actions), axis=1, keepdims=True)\n",
    "            entropy = tf.reduce_sum(dist.entropy(), axis=1, keepdims=True)\n",
    "            \n",
    "            # Critic forward pass\n",
    "            values = self.critic(states)\n",
    "            td_error = target_values - values\n",
    "            \n",
    "            # Define losses\n",
    "            actor_loss = -tf.reduce_mean(log_prob * tf.stop_gradient(td_error) + beta * entropy)\n",
    "            critic_loss = tf.reduce_mean(tf.square(td_error))\n",
    "        \n",
    "        # Compute gradients\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        \n",
    "        # Clip gradients\n",
    "        actor_grads = [tf.clip_by_norm(grad, 40) for grad in actor_grads if grad is not None]\n",
    "        critic_grads = [tf.clip_by_norm(grad, 40) for grad in critic_grads if grad is not None]\n",
    "        \n",
    "        return actor_grads, critic_grads\n",
    "        \n",
    "    \n",
    "    def work(self, coordinator):\n",
    "        global global_rewards, global_episodes\n",
    "        episode = 0\n",
    "        \n",
    "        while episode < num_episodes and not coordinator.should_stop():\n",
    "            state, _ = self.env.reset()\n",
    "            batch_states, batch_actions, batch_rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "            \n",
    "            for t in range(num_timesteps):\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                \n",
    "                done = True if t == num_timesteps - 1 else False\n",
    "                episode_reward += reward\n",
    "                \n",
    "                batch_states.append(state)\n",
    "                batch_actions.append(action)\n",
    "                batch_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    if done:\n",
    "                        target = 0\n",
    "                    else:\n",
    "                        target = self.critic(tf.convert_to_tensor([next_state], dtype=tf.float32))\n",
    "                        target = target.numpy()[0, 0]\n",
    "                    \n",
    "                    # Target values\n",
    "                    batch_target_value = []\n",
    "                    for reward in batch_rewards[::-1]:\n",
    "                        target = reward + gamma * target\n",
    "                        batch_target_value.append(target)\n",
    "                    batch_target_value.reverse()\n",
    "                    \n",
    "                    # Compute gradients and add to queue\n",
    "                    gradients = self.compute_gradients(\n",
    "                        np.vstack(batch_states),\n",
    "                        np.vstack(batch_actions),\n",
    "                        np.vstack(batch_target_value)\n",
    "                    )\n",
    "                    self.gradient_queue.put(gradients)\n",
    "                    \n",
    "                    # Update episode stats\n",
    "                    global_rewards.append(episode_reward)\n",
    "                    episode += 1\n",
    "                    \n",
    "                    if (episode+1) % 10 == 0:\n",
    "                        print(\n",
    "                            self.name,\n",
    "                            \"Epispde:\", episode+1,\n",
    "                            \"| Reward: %i\" % global_rewards[-1]\n",
    "                            )\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "# Initialize global agent and gradient queue\n",
    "global_agent = GlobalAgent(env, (250, 250))\n",
    "gradient_queue = Queue()\n",
    "global_rewards = []\n",
    "global_episodes = 0\n",
    "\n",
    "# Create coordinator for managing workers\n",
    "coordinator = tf.train.Coordinator()\n",
    "\n",
    "# Initialize workers\n",
    "workers = [A2CWorker(f'W_{i}', global_agent, gradient_queue) \n",
    "          for i in range(num_workers)]\n",
    "\n",
    "# Create worker threads\n",
    "worker_threads = []\n",
    "for worker in workers:\n",
    "    thread = threading.Thread(target=worker.work, args=(coordinator,))\n",
    "    thread.start()\n",
    "    worker_threads.append(thread)\n",
    "    # worker.work(coordinator)\n",
    "    \n",
    "try:\n",
    "    while global_episodes < num_episodes:\n",
    "        # Wait for all workers to submit their gradients\n",
    "        all_gradients = []\n",
    "        for _ in range(num_workers):\n",
    "            gradients = gradient_queue.get()\n",
    "            all_gradients.append(gradients)\n",
    "        \n",
    "        # Perform synchronous update\n",
    "        global_agent.update(all_gradients)\n",
    "        \n",
    "        # Synchronize all workers with updated global networks\n",
    "        for worker in workers:\n",
    "            worker.sync_with_global()\n",
    "        \n",
    "        global_episodes += num_workers\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    coordinator.request_stop()\n",
    "\n",
    "coordinator.request_stop()\n",
    "coordinator.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(policy_model):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = \"Renders/A2C_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        action = policy_model.select_action(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(workers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLC_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
