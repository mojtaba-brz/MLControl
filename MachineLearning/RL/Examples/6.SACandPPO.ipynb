{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Soft Policy Gradient Methods\n",
    "\n",
    "#### Refs: \n",
    "- [Sharif University of Technology - Deep Reinforcement Learning (Fall 2024) - Dr.A.Emami and M.Narimani](https://github.com/mnarimani/DRL_Fall2024_SUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "from Lib.ReplayBuffer import ReplayBuffer2\n",
    "from Lib.ActorCritic import ValueNetwork, ActorNetwork, CriticNetwork\n",
    "\n",
    "env_name = 'Pendulum-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(self, alpha=0.0003, beta=0.0003, input_dims=[8],\n",
    "            env=None, gamma=0.99, n_actions=2, max_size=1000000, tau=0.005,\n",
    "            layer1_size=256, layer2_size=256, batch_size=256, reward_scale=2):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer2(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Initialize temperature parameter (alpha) and its optimizer\n",
    "        self.log_alpha = tf.Variable(tf.math.log(0.2), dtype=tf.float32)\n",
    "        self.alpha_T = tf.exp(self.log_alpha)\n",
    "        self.alpha_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "        # Optional: Set target entropy (can also be set in learn method)\n",
    "        self.target_entropy = -tf.cast(self.n_actions, dtype=tf.float32)\n",
    "\n",
    "        self.actor = ActorNetwork(env, (250, 250), name='actor')\n",
    "        self.critic_1 = CriticNetwork(env, (256, 256), name='critic_1')\n",
    "        self.critic_2 = CriticNetwork(env, (256, 256), name='critic_2')\n",
    "        self.value = CriticNetwork(env, (256, 256), name='value')\n",
    "        self.target_value = CriticNetwork(env, (256, 256), name='target_value')\n",
    "\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
    "        self.critic_1.compile(optimizer=Adam(learning_rate=beta))\n",
    "        self.critic_2.compile(optimizer=Adam(learning_rate=beta))\n",
    "        self.value.compile(optimizer=Adam(learning_rate=beta))\n",
    "        self.target_value.compile(optimizer=Adam(learning_rate=beta))\n",
    "\n",
    "        self.scale = reward_scale\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
    "\n",
    "        return actions[0]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store(state, action, reward, new_state, done)\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_value.weights\n",
    "        for i, weight in enumerate(self.value.weights):\n",
    "            weights.append(weight * tau + targets[i]*(1-tau))\n",
    "\n",
    "        self.target_value.set_weights(weights)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.m_cntr < self.batch_size:\n",
    "            return 0, 0, 0, 0\n",
    "\n",
    "        state, action, reward, new_state, done = \\\n",
    "                self.memory.sample(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        # Update temperature parameter\n",
    "\n",
    "        # target_entropy = -tf.cast(tf.shape(action)[-1], dtype=tf.float32)\n",
    "        # with tf.GradientTape() as tape:\n",
    "        #     _, log_probs = self.actor.sample_normal(states, reparameterize=True)\n",
    "        #     log_probs = tf.squeeze(log_probs, 1)\n",
    "        #     alpha_loss = -tf.reduce_mean(\n",
    "        #         self.log_alpha * tf.stop_gradient(log_probs + target_entropy))\n",
    "        #\n",
    "        # alpha_gradient = tape.gradient(alpha_loss, [self.log_alpha])\n",
    "        # self.alpha_optimizer.apply_gradients(zip(alpha_gradient, [self.log_alpha]))\n",
    "        # self.alpha_T = tf.exp(self.log_alpha)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = self.value(states)\n",
    "            value_ = self.target_value(states_)\n",
    "\n",
    "            current_policy_actions, log_probs = self.actor.sample_normal(states,\n",
    "                                                        reparameterize=False)\n",
    "            log_probs = tf.squeeze(log_probs,1)\n",
    "            x = tf.concat([states, current_policy_actions], axis=-1)\n",
    "            q1_new_policy = self.critic_1(x)\n",
    "            q2_new_policy = self.critic_2(x)\n",
    "            critic_value = tf.squeeze(tf.math.minimum(q1_new_policy, q2_new_policy))\n",
    "\n",
    "            value_target = critic_value - self.alpha_T * log_probs\n",
    "            value_loss = 0.5 * keras.losses.MSE(value, value_target)\n",
    "\n",
    "        value_network_gradient = tape.gradient(value_loss, \n",
    "                                                self.value.trainable_variables)\n",
    "        self.value.optimizer.apply_gradients(zip(\n",
    "                       value_network_gradient, self.value.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_policy_actions, log_probs = self.actor.sample_normal(states,\n",
    "                                                reparameterize=True)\n",
    "            log_probs = tf.squeeze(log_probs, 1)\n",
    "            x = tf.concat([states, new_policy_actions], axis=-1)\n",
    "            q1_new_policy = self.critic_1(x)\n",
    "            q2_new_policy = self.critic_2(x)\n",
    "            critic_value = tf.math.minimum(q1_new_policy, q2_new_policy)\n",
    "        \n",
    "            actor_loss = self.alpha_T * log_probs - critic_value\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "            log_probs_2 = log_probs\n",
    "\n",
    "        actor_network_gradient = tape.gradient(actor_loss, \n",
    "                                            self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(\n",
    "                        actor_network_gradient, self.actor.trainable_variables))\n",
    "        \n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            q_hat = self.scale*reward + self.gamma*value_*(1-done)\n",
    "            x = tf.concat([state, action], axis=-1)\n",
    "            q1_old_policy = self.critic_1(x)\n",
    "            q2_old_policy = self.critic_2(x)\n",
    "            critic_1_loss = 0.5 * keras.losses.MSE(q1_old_policy, q_hat)\n",
    "            critic_2_loss = 0.5 * keras.losses.MSE(q2_old_policy, q_hat)\n",
    "    \n",
    "        critic_1_network_gradient = tape.gradient(critic_1_loss,\n",
    "                                        self.critic_1.trainable_variables)\n",
    "        critic_2_network_gradient = tape.gradient(critic_2_loss,\n",
    "            self.critic_2.trainable_variables)\n",
    "\n",
    "        self.critic_1.optimizer.apply_gradients(zip(\n",
    "            critic_1_network_gradient, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(\n",
    "            critic_2_network_gradient, self.critic_2.trainable_variables))\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "        return value_loss, actor_loss, critic_1_loss, critic_2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "agent = SACAgent(input_dims=env.observation_space.shape, env=env,\n",
    "              n_actions=env.action_space.shape[0])\n",
    "n_episodes = 150\n",
    "\n",
    "score_history = []\n",
    "value_losses = []\n",
    "actor_losses = []\n",
    "critic_1_losses = []\n",
    "critic_2_losses = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    while not done and steps < 200:\n",
    "        steps += 1\n",
    "        action = agent.choose_action(observation)\n",
    "\n",
    "        observation_, reward, done, info, _ = env.step(action.numpy() * 2)  # action output is in [-1,1]\n",
    "        score += reward\n",
    "        agent.remember(observation, action, reward, observation_, done)\n",
    "        value_loss, actor_loss, critic_1_loss, critic_2_loss = agent.learn()\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "\n",
    "    value_losses.append(value_loss)\n",
    "    actor_losses.append(actor_loss)\n",
    "    critic_1_losses.append(critic_1_loss)\n",
    "    critic_2_losses.append(critic_2_loss)\n",
    "\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if (i+1) % 5 == 0:\n",
    "        print(f\"Episode: {i+1:4d} | \"\n",
    "              f\"Score: {int(score):5d} | \"\n",
    "              f\"Avg Score: {int(avg_score):5d} | \"\n",
    "              f\"Actor Loss: {actor_loss:.2f} | \"\n",
    "              f\"Critic 1 Loss: {critic_1_loss:.2f} | \"\n",
    "              f\"Critic 2 Loss: {critic_2_loss:.2f} | \"\n",
    "              f\"Value Loss: {value_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(agent):\n",
    "    num_timesteps = 200\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = \"Renders/SAC_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        # action = env.action_space.sample()\n",
    "        action = agent.choose_action(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### PPO :   Proximal Policy Optimization (PPO-Clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "env = gym.make(env_name)\n",
    "S_DIM = env.observation_space.shape[0]\n",
    "A_DIM = env.action_space.shape[0]\n",
    "A_BOUND = [env.action_space.low[0], env.action_space.high[0]]\n",
    "EP_MAX = 2000\n",
    "EP_LEN = 200\n",
    "GAMMA = 0.9\n",
    "A_LR = 0.0001\n",
    "C_LR = 0.0005\n",
    "BATCH = 64\n",
    "A_UPDATE_STEPS = 10\n",
    "C_UPDATE_STEPS = 10\n",
    "EPSILON = 0.2 # Clipped surrogate objective\n",
    "\n",
    "class PPOActorNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.mu = tf.keras.layers.Dense(A_DIM, activation='tanh')\n",
    "        self.sigma = tf.keras.layers.Dense(A_DIM, activation='softplus')\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        mu = self.mu(x) * A_BOUND[1]\n",
    "        sigma = self.sigma(x)\n",
    "        return mu, sigma + 1e-4\n",
    "    \n",
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.value = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        return self.value(x)\n",
    "        \n",
    "class PPO:\n",
    "    def __init__(self):\n",
    "        self.actor = PPOActorNetwork()\n",
    "        self.actor_old = PPOActorNetwork()\n",
    "        self.critic = CriticNetwork()\n",
    "        \n",
    "        # Build models with dummy input\n",
    "        dummy_state = tf.random.normal((1, S_DIM))\n",
    "        self.actor(dummy_state)\n",
    "        self.actor_old(dummy_state)\n",
    "        self.critic(dummy_state)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=A_LR)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=C_LR)\n",
    "        \n",
    "        # Add metrics tracking\n",
    "        self.actor_loss_metric = tf.keras.metrics.Mean('actor_loss', dtype=tf.float32)\n",
    "        self.critic_loss_metric = tf.keras.metrics.Mean('critic_loss', dtype=tf.float32)\n",
    "\n",
    "    def update_old_actor(self):\n",
    "        self.actor_old.set_weights(self.actor.get_weights())\n",
    "\n",
    "    @tf.function\n",
    "    def choose_action(self, state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        mu, sigma = self.actor(state)\n",
    "        dist = tf.random.normal(shape=mu.shape)\n",
    "        action = mu + sigma * dist\n",
    "        return tf.clip_by_value(action[0], A_BOUND[0], A_BOUND[1])\n",
    "\n",
    "    @tf.function\n",
    "    def get_value(self, state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        return self.critic(state)[0, 0]\n",
    "\n",
    "    @tf.function\n",
    "    def actor_loss(self, states, actions, advantages):\n",
    "        mu, sigma = self.actor(states)\n",
    "        old_mu, old_sigma = self.actor_old(states)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        dist = tfp.distributions.Normal(mu, sigma)\n",
    "        old_dist = tfp.distributions.Normal(old_mu, old_sigma)\n",
    "        \n",
    "        ratio = tf.exp(dist.log_prob(actions) - old_dist.log_prob(actions))\n",
    "        surr = ratio * advantages\n",
    "        \n",
    "        # Clipped surrogate objective\n",
    "        clip_surr = tf.clip_by_value(ratio, 1.-EPSILON, 1.+EPSILON) * advantages\n",
    "        \n",
    "        return -tf.reduce_mean(tf.minimum(surr, clip_surr))\n",
    "\n",
    "    @tf.function\n",
    "    def critic_loss(self, states, discounted_rewards):\n",
    "        values = self.critic(states)\n",
    "        return tf.reduce_mean(tf.square(discounted_rewards - values))\n",
    "\n",
    "    @tf.function\n",
    "    def train_actor(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.actor_loss(states, actions, advantages)\n",
    "        grads = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "        self.actor_loss_metric.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_critic(self, states, discounted_rewards):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.critic_loss(states, discounted_rewards)\n",
    "        grads = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "        self.critic_loss_metric.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    def update(self, states, actions, rewards):\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        \n",
    "        # Reset metrics\n",
    "        self.actor_loss_metric.reset_state()\n",
    "        self.critic_loss_metric.reset_state()\n",
    "        \n",
    "        # Calculate advantage\n",
    "        values = self.critic(states)\n",
    "        advantages = rewards - values\n",
    "        \n",
    "        # Update old actor\n",
    "        self.update_old_actor()\n",
    "        \n",
    "        # Update actor\n",
    "        for _ in range(A_UPDATE_STEPS):\n",
    "            self.train_actor(states, actions, advantages)\n",
    "            \n",
    "        # Update critic\n",
    "        for _ in range(C_UPDATE_STEPS):\n",
    "            self.train_critic(states, rewards)\n",
    "            \n",
    "        # Return the average losses\n",
    "        return {\n",
    "            'actor_loss': self.actor_loss_metric.result().numpy(),\n",
    "            'critic_loss': self.critic_loss_metric.result().numpy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO()\n",
    "all_ep_r = []\n",
    "all_actor_losses = []\n",
    "all_critic_losses = []\n",
    "\n",
    "for ep in range(EP_MAX):\n",
    "    s = env.reset()[0]\n",
    "    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "    ep_r = 0\n",
    "    ep_actor_losses = []\n",
    "    ep_critic_losses = []\n",
    "    \n",
    "    for t in range(EP_LEN):\n",
    "        a = ppo.choose_action(s).numpy()\n",
    "        s_, r, terminated, truncated, _ = env.step(a)\n",
    "        \n",
    "        buffer_s.append(s)\n",
    "        buffer_a.append(a)\n",
    "        buffer_r.append(r)\n",
    "        \n",
    "        s = s_\n",
    "        ep_r += r\n",
    "\n",
    "        # update ppo\n",
    "        if (t + 1) % BATCH == 0 or t == EP_LEN - 1:\n",
    "            v_s_ = ppo.get_value(s_).numpy()\n",
    "            discounted_r = []\n",
    "            for r in buffer_r[::-1]:\n",
    "                v_s_ = r + GAMMA * v_s_\n",
    "                discounted_r.append(v_s_)\n",
    "            discounted_r.reverse()\n",
    "\n",
    "            bs = np.vstack(buffer_s)\n",
    "            ba = np.vstack(buffer_a)\n",
    "            br = np.array(discounted_r)[:, np.newaxis]\n",
    "            \n",
    "            losses = ppo.update(bs, ba, br)\n",
    "            ep_actor_losses.append(losses['actor_loss'])\n",
    "            ep_critic_losses.append(losses['critic_loss'])\n",
    "            buffer_s, buffer_a, buffer_r = [], [], []\n",
    "\n",
    "\n",
    "    all_ep_r.append(ep_r)\n",
    "    avg_score = np.mean(all_ep_r[-100:])\n",
    "    \n",
    "    avg_actor_loss = np.mean(ep_actor_losses) if ep_actor_losses else 0\n",
    "    avg_critic_loss = np.mean(ep_critic_losses) if ep_critic_losses else 0\n",
    "    all_actor_losses.append(avg_actor_loss)\n",
    "    all_critic_losses.append(avg_critic_loss)\n",
    "\n",
    "    if (ep+1) % 10 == 0:\n",
    "        print(f\"Episode: {ep+1:4d} | \"\n",
    "              f\"Score: {int(ep_r):5d} | \"\n",
    "              f\"Avg Score: {int(avg_score):5d} | \"\n",
    "              f\"Actor Loss: {avg_actor_loss:.2f} | \"\n",
    "              f\"Critic Loss: {avg_critic_loss:.2f}\")\n",
    "\n",
    "    # Check if solved\n",
    "    if len(all_ep_r) >= 100 and avg_score >= -300:\n",
    "        print(f'Problem solved in {ep+1} episodes')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "import cv2\n",
    "\n",
    "def render_pendulum(agent):\n",
    "    num_timesteps = 400\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    video_path = \"Renders/PPO_render.mp4\"\n",
    "    frame_width, frame_height = env.render().shape[1], env.render().shape[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    out = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))\n",
    "    for _ in range(num_timesteps):\n",
    "        # action = env.action_space.sample()\n",
    "        action = agent.choose_action(state).numpy()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        frame = env.render()\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    env.close()\n",
    "    display(Video(video_path))\n",
    "\n",
    "render_pendulum(ppo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLC_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
