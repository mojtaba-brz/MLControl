{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial for Policy Gradients\n",
    "> Deep RL course - Fall 2024 - Sharif University of Technology - Workshop session <br>\n",
    "> Author: M Narimani - December 8th, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Basic Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors\n",
    "x = tf.constant([1.0, 2.0, 3.0])\n",
    "y = tf.constant([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"x:\", x.numpy())\n",
    "print(\"y:\", y.numpy())\n",
    "print(\"\\nBasic operations:\")\n",
    "print(\"Addition:\", (x + y).numpy())\n",
    "print(\"Multiplication:\", (x * y).numpy())\n",
    "print(\"Mean of x:\", tf.reduce_mean(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations\n",
    "A = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = tf.constant([[5.0], [6.0]])\n",
    "\n",
    "print(\"Matrix multiplication:\")\n",
    "print(\"\\nA:\")\n",
    "print(A.numpy())\n",
    "print(\"\\nb:\")\n",
    "print(b.numpy())\n",
    "print(\"\\nA @ b:\")\n",
    "print((A @ b).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Probability Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits (raw/non-normalized predictions) to probabilities\n",
    "logits = tf.constant([2.0, 1.0, 0.5])\n",
    "probs = tf.nn.softmax(logits)\n",
    "print(\"Logits:\", logits.numpy())\n",
    "print(\"Probabilities (softmax):\", probs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log probabilities\n",
    "log_probs = tf.math.log(probs)\n",
    "print(\"Log probabilities:\", log_probs.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gradient example\n",
    "x = tf.Variable(2.0)\n",
    "z = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * x + z\n",
    "grad = tape.gradient(y, x)\n",
    "print(\"dy/dx at x=2:\", grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple variables and operations\n",
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x1 * x2 + tf.square(x1)\n",
    "grads = tape.gradient(y, [x1, x2])\n",
    "print(\"Gradients for x1 and x2:\", [g.numpy() for g in grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a grid of points\n",
    "x = np.linspace(-4, 4, 40)\n",
    "y = np.linspace(-4, 4, 40)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Convert to TensorFlow Variables\n",
    "x_tf = tf.Variable(X)\n",
    "y_tf = tf.Variable(Y)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tf.sin(x_tf)**2 + tf.cos(y_tf)**2\n",
    "\n",
    "grads = tape.gradient(z, [x_tf, y_tf])\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, z.numpy(), cmap='viridis')\n",
    "ax.set_title('Function Surface')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "# Gradient vector field\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.quiver(X, Y, grads[0].numpy(), grads[1].numpy())\n",
    "plt.title('Gradient Vector Field')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Policy gradient precess\n",
    "## Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(8, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Data Collection Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_cartpole_state():\n",
    "    return np.random.randn(4)  # [cart_position, cart_velocity, pole_angle, pole_velocity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "raw_probs = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    state = simulate_cartpole_state()\n",
    "    states.append(state)\n",
    "    \n",
    "    # Get action probabilities\n",
    "    state_input = np.array([state])\n",
    "    action_probs = model(state_input).numpy().flatten()\n",
    "    raw_probs.append(action_probs)\n",
    "    \n",
    "    # Sample action\n",
    "    action = np.random.choice(2, p=action_probs)\n",
    "    actions.append(action)\n",
    "    \n",
    "    print(f\"\\nStep {step + 1}:\")\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"Action probabilities: [left={action_probs[0]:.2f}, right={action_probs[1]:.2f}]\")\n",
    "    print(f\"Chosen action: {'left' if action == 0 else 'right'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Reward Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    \"\"\"Calculate discounted rewards\"\"\"\n",
    "    discounted = np.zeros_like(rewards, dtype=np.float32)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        running_sum = running_sum * gamma + rewards[t]\n",
    "        discounted[t] = running_sum\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example rewards\n",
    "raw_rewards = [1.0, 1.0, 1.0]\n",
    "discounted_rewards = discount_rewards(raw_rewards)\n",
    "\n",
    "print(\"Raw rewards:\", raw_rewards)\n",
    "print(\"Discounted rewards:\", discounted_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Policy Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array(states)\n",
    "actions = np.array(actions)\n",
    "\n",
    "# Create action indices\n",
    "idx = np.array(list(zip(range(len(actions)), actions)))\n",
    "print(\"Action indices (idx):\")\n",
    "print(idx)\n",
    "print(\"\\nEach row is (timestep, action_taken)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy gradient update\n",
    "with tf.GradientTape() as tape:\n",
    "    # Get action probabilities for all states\n",
    "    action_probs = model(states)\n",
    "    \n",
    "    # Select probabilities of actions that were taken\n",
    "    selected_probs = tf.gather_nd(action_probs, idx) #gather_nd returnes the elements of action_probs corresponding to indices of idx\n",
    "    print(\"\\nProbabilities of selected actions:\", selected_probs.numpy())\n",
    "    \n",
    "    # Calculate log probabilities\n",
    "    log_probs = tf.math.log(selected_probs)\n",
    "    print(\"Log probabilities:\", log_probs.numpy())\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = -tf.reduce_mean(log_probs * discounted_rewards)\n",
    "    print(\"\\nPolicy gradient loss:\", loss.numpy())\n",
    "\n",
    "# Get gradients\n",
    "gradients = tape.gradient(loss, model.trainable_variables)\n",
    "print(\"\\nGradient shapes:\", [g.shape for g in gradients])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "> **TODO:** Justify \"Gradient shapes\" by calling `model.summary()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple value network\n",
    "baseline = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(8, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Predict values\n",
    "values = baseline(states)\n",
    "print(\"Predicted values:\", values.numpy().flatten())\n",
    "\n",
    "# Calculate advantages\n",
    "advantages = discounted_rewards - values.numpy().flatten()\n",
    "print(\"Advantages:\", advantages)\n",
    "\n",
    "# Demonstrate policy update with advantages\n",
    "with tf.GradientTape() as tape:\n",
    "    action_probs = model(states)\n",
    "    selected_probs = tf.gather_nd(action_probs, idx)\n",
    "    log_probs = tf.math.log(selected_probs)\n",
    "    \n",
    "    # Use advantages instead of raw rewards\n",
    "    loss = -tf.reduce_mean(log_probs * advantages)\n",
    "    print(\"\\nPolicy gradient loss with advantages:\", loss.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLC_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
